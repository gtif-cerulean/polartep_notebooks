{"version":"1","records":[{"hierarchy":{"lvl1":"Example Viewer for PolarTEP Notebooks"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Example Viewer for PolarTEP Notebooks"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Sentinel-3 Snow and ICE products (SICE)"},"type":"lvl1","url":"/notebooks/sice","position":0},{"hierarchy":{"lvl1":"Sentinel-3 Snow and ICE products (SICE)"},"content":"#%run ./prepare-sice-environment.ipynb\n! pip install --upgrade sentinelhub \n\n\n\n\n## Credentials needs to be filled in the code in order to make it work!\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport logging\nimport subprocess\nfrom botocore.client import Config as botoConfig\nfrom botocore.exceptions import ClientError\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\nimport boto3\nimport glob\nimport time\nimport rasterio\nfrom shapely import geometry\nfrom osgeo import ogr\n\nlogging = logging.getLogger(__name__)\n\ndef plot_image(image, factor=1.0, clip_range=None, **kwargs):\n    \"\"\"\n    Utility function for plotting RGB images.\n    \"\"\"\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n    if clip_range is not None:\n        ax.imshow(np.clip(image * factor, *clip_range), **kwargs)\n    else:\n        ax.imshow(image * factor, **kwargs)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    \ndef merge_tiffs(input_filename_list, merged_filename, *, overwrite=False, delete_input=False):\n    \"\"\"Performs gdal_merge on a set of given geotiff images\n\n    :param input_filename_list: A list of input tiff image filenames\n    :param merged_filename: Filename of merged tiff image\n    :param overwrite: If True overwrite the output (merged) file if it exists\n    :param delete_input: If True input images will be deleted at the end\n    \"\"\"\n    if os.path.exists(merged_filename):\n        if overwrite:\n            os.remove(merged_filename)\n        else:\n            raise OSError(f\"{merged_filename} exists!\")\n\n    logging.info(\"merging %d tiffs to %s\", len(input_filename_list), merged_filename)\n    subprocess.check_call(\n        [\"gdal_merge.py\", \"-co\", \"BIGTIFF=YES\", \"-co\", \"compress=LZW\", \"-ot\", \"Float32\" , \"-init\", \"-9999\" , \"-n\", \"-9999\", \"-a_nodata\", \"-9999\", \"-o\", merged_filename, *input_filename_list]\n    )\n    logging.info(\"merging done\")\n\n    if delete_input:\n        logging.info(\"deleting input files\")\n        for filename in input_filename_list:\n            if os.path.isfile(filename):\n                os.remove(filename)\n                \ndef deleteProcessingData(dir):\n    logging.info(f'Deleting files from {dir}')\n    for file in glob.glob(f'{dir}/*_tmp.tif'):\n        logging.info(f'Deleting file {file}')\n        os.remove(file)\n\ndef importToBucket(awsConfig, resultsFolder, processBands=False):\n    \n    try:\n        s3_client = boto3.resource('s3',\n                                   endpoint_url=awsConfig['s3Url'],\n                                   use_ssl=False,\n                                   aws_access_key_id=awsConfig['s3AccessKey'],\n                                   aws_secret_access_key=awsConfig['s3SecrestKey'],\n                                   config=botoConfig(\n                                       signature_version='s3',\n                                       connect_timeout=60,\n                                       read_timeout=60,\n                                   ))\n\n        bucket = s3_client.Bucket(awsConfig['bucketName'])\n    except Exception as e:\n        logging.error(e)\n        raise Exception('Invalid bucket parameters') \n    \n    coverGeometry = ''\n    \n    # get tif files to upload\n    filenamesList = glob.glob(f'{resultsFolder}/*.tif')\n    try:\n        for file in filenamesList:\n            destFile = file.replace(resultsFolder, awsConfig['folder'])\n            if processBands:\n                tmpFile = file.replace(\".tif\", \"_tmp.tif\")\n                cmd = \"gdal_translate -b 1 -of COG -a_nodata -9999 -co COMPRESS=DEFLATE -co BLOCKSIZE=1024 -co RESAMPLING=AVERAGE -co OVERVIEWS=IGNORE_EXISTING {0} {1}\".format(file, tmpFile)\n                logging.info(cmd)\n                os.system(cmd)\n                file = tmpFile\n                \n            if not coverGeometry:\n                coverGeometry = getMbrWithIntermediate(file)\n            \n            logging.info(f'Uploading {file} to {destFile}')\n            response = bucket.upload_file(file, destFile)\n        deleteProcessingData(resultsFolder)\n    except Exception as e:\n        logging.error(e)\n        raise Exception(f'Failed to upload file: {destFile}') \n        \n    return coverGeometry\n\ndef addIntermediateY(x, y, diff, numPoints):\n    coords = []\n    coords.append([x, y])  \n    for i in range(numPoints):\n        coords.append([x, y + i / numPoints * diff])\n    return coords\n\ndef addIntermediateX(x, y, diff, numPoints):\n    coords = []\n    coords.append([x, y])  \n    for i in range(numPoints):\n        coords.append([x + i / numPoints * diff, y])\n    return coords\n\ndef getMbrWithIntermediate(file):\n    dataset = rasterio.open(file)\n    bounds = dataset.bounds\n    poly = getPolygonFromMbr(bounds.left, bounds.bottom, bounds.right, bounds.top, 20)\n    p = geometry.mapping(poly)\n    p['properties'] = {'name': 'urn:ogc:def:crs:EPSG::4326'}\n    return p         \n        \ndef getPolygonFromMbr(xMin, yMin, xMax, yMax, numPoints):\n    coords = []\n    xDiff = xMax - xMin\n    yDiff = yMax - yMin\n    \n    coords.extend(addIntermediateY(xMin, yMin, yDiff, numPoints))\n    coords.extend(addIntermediateX(xMin, yMax, xDiff, numPoints))\n    coords.extend(addIntermediateY(xMax, yMax, -yDiff, numPoints))\n    coords.extend(addIntermediateX(xMax, yMin, -xDiff, numPoints))\n    poly = geometry.Polygon(coords)\n    return poly        \n        \ndef deleteTile(s3folder, oauth, byocUrl):\n    url = f'{byocUrl}/tiles'\n\n    try:\n        while url is not None:\n            print(f'Calling url: {url}')\n            response = oauth.get(url)\n            response.raise_for_status()\n\n            output = response.json()\n            tiles = output['data']\n            links = output['links']\n\n            for tile in tiles:\n                if tile['path'].startswith(s3folder):\n                    tile_id = tile['id']\n                    response = oauth.delete(f'{byocUrl}/tiles/{tile_id}')\n                    print(\"Deleted tile: \" + tile['path'])\n                    try:\n                        response.raise_for_status()\n                    except Exception as e:\n                        print(\"Failed to delete tile {0}: {1}\".format(\n                            tile_id, response.reason))\n                        logging.error(e)\n\n            # sets url to None if there's no link to the next set of tiles\n            url = links.get('next', None)\n\n            # waits a bit before fetching the next set\n            time.sleep(0.1)\n    except Exception as e:\n        logging.error(\"BYOC delete error: {0}\".format(response.reason))\n        logging.error(e)\n        raise\n\ndef insertTile(tile, oauth, byocUrl):\n    print(f'Inserting tile {tile}')\n    try:\n        response = oauth.post(f'{byocUrl}/tiles', json=tile)\n        response.raise_for_status()\n    except Exception as e:\n        logging.error(\"BYOC import error: {0}\".format(response.reason))\n        logging.error(e)\n\n\ndef importToBYOC(config):\n     # Create a session\n    client = BackendApplicationClient(client_id=config['client_id'])\n    oauth = OAuth2Session(client=client)\n    oauth.fetch_token(token_url='https://services.sentinel-hub.com/oauth/token',\n                      client_id=['client_id'], client_secret=config['client_secret'])\n\n    byoc_service_base_url = config['byoc_service_base_url']\n    collection_id = config['collection_id']\n    byocUrl = f'{byoc_service_base_url}/collections/{collection_id}'\n    \n    # ingest tile\n    tilePath = config['folder']\n    tile = {\n        'path': tilePath,\n        'sensingTime': config['sensingTime'],\n        'coverGeometry' : config['coverGeometry']\n    }\n    \n    if config['coverGeometry']: \n         tile ['coverGeometry'] = config['coverGeometry']\n            \n    logging.info(\"Deleting folder: \" + tilePath)\n    deleteTile(tilePath, oauth, byocUrl)\n    logging.info(\"Ingesting folder: \" + tilePath)\n    insertTile(tile, oauth, byocUrl)\n    \n    \ndef defaultBYOCImport(resultsFolder, s3Folder, resolution, sensingTime):\n    if not (resolution == 300 or resolution == 500) :\n        logger.info('Resolution not supported.')\n        return\n    \n    s3config = {\n        's3Url' : 'https://s3.waw2-1.cloudferro.com',\n        's3AccessKey' : 'xxx',\n        's3SecrestKey' : 'xxx',\n        'bucketName' : 'polar',\n        'folder' : s3Folder,\n    }\n    coverGeometry = importToBucket(s3config, resultsFolder, True)\n    \n    collectionID_300m = 'd34c470c-52a8-49db-9f9b-0956f10734d9'\n    collectionID_500m = '6a3a4f71-84ff-4421-95a7-6ba969b5cf88'\n    \n    collectionId = collectionID_300m if resolution == 300 else collectionID_500m\n    \n    shConfig = {\n        'byoc_service_base_url' : 'https://creodias.sentinel-hub.com/api/v1/byoc',\n        'client_id' : 'xxx',\n        'client_secret' : 'xxx',\n        'collection_id' : collectionId,\n        'folder' : s3Folder,\n        'sensingTime' : sensingTime,\n        'coverGeometry' : coverGeometry\n    }    \n    importToBYOC(shConfig)\n\n\n\n# Various utilities\nimport os\nimport json\nfrom shapely import geometry, wkt\nimport datetime as dt\nimport numpy as np\nimport geopandas as gpd\nimport glob\nimport shutil\nimport logging\nimport concurrent.futures\nimport time\n\nfrom sentinelhub import SentinelHubDownloadClient, SentinelHubBatch, SentinelHubRequest, Geometry, CRS, DataCollection, MimeType, SHConfig, BBox, bbox_to_dimensions\nfrom shapely.geometry import Polygon\nimport tarfile\n\n# create logs folder\nif not os.path.exists(\"logs\"):\n    os.makedirs(\"logs\")\n\n# right now we only log to consol\nlogging.basicConfig(\n    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n    level=logging.INFO,\n    datefmt='%Y-%m-%d %H:%M:%S',\n    handlers=[\n        logging.FileHandler(f'logs/sice_sh_{time.strftime(\"%Y_%m_%d\",time.localtime())}.log'),\n        logging.StreamHandler()\n    ])\n\n\n\n#External variables\n# Set the date of calculation\ndate = \"2021-07-09\"\n\n# resolution (m) \nresolution = 1200  # minimum resolution of data is 300m\n\n#area of interest\naoi = 'POLYGON((-53.6565 82.4951, -59.9608 82.1309, -67.7892 80.5602, -67.9606 80.0218, -67.6072 79.3014, -72.7375 78.5894, -73.5413 78.1636, -72.9428 77.3837, -69.0700 76.0128, -66.6509 75.7624, -60.3956 75.8231, -58.4311 74.8854, -55.1967 69.6980, -53.8565 68.8368, -54.2986 67.0754, -53.5562 65.6109, -52.3863 64.7989, -52.3228 64.0074, -50.2076 62.1010, -48.6300 60.7381, -45.0522 59.7674, -43.2890 59.6436, -42.4957 60.3093, -41.8486 61.5655, -41.6969 62.6486, -40.1106 63.5452, -39.9111 64.7944, -38.0777 65.4068, -36.9899 65.1987, -31.2165 67.7166, -25.8502 68.6303, -21.6517 70.0839, -20.9932 70.7880, -21.2829 72.9254, -16.9050 74.9601, -17.1213 79.6158, -10.2883 81.4244, -14.0398 81.9745, -17.8112 82.0131, -28.5252 83.7013, -40.1075 83.6651, -53.6565 82.4951))'\naoi = \"POLYGON ((-14.675905 65.792421, -14.675905 66.238873, -13.423464 66.238873, -13.423464 65.792421, -14.675905 65.792421))\"\n#sub-area\n#aoi = 'POLYGON ((-56.045995 66.271911, -49.369389 66.271911, -49.369389 71.949832, -56.045995 71.949832, -56.045995 66.271911))'\n\n#Island\n#aoi = 'POLYGON ((-24.634498 66.588207, -13.38895 66.588207, -13.38895 63.292939, -24.634498 63.292939, -24.634498 66.588207))'\n\n#target projection of the final results\nprojection = '4326' #default\n\n#projection = '3413'  #polar\n \n\n# log processing parameters - don't log any S3 information\nlogging.info(f'Date: {date}')\nlogging.info(f'AOI: {aoi}')\nlogging.info(f'Projection: {projection}')\n\n\n\n\n\n#verify input parameters\n\nif not date:\n    raise Exception('variable date has to be set') \n\nif not aoi:\n    raise Exception('aoi has to be set')     \n\nif not resolution:\n    raise Exception('resolution has to be set')     \n\nif resolution < 300 or resolution > 1200: \n    raise Exception('value of resolution has to be between 200 m and 1200 m')\n\n#transform period into single day    \nif '/' in date:\n    date = date[0:date.find('/')]    \n\n\n\n#system settings\n\n#base folder where the code is located\nUSR_PATH = os.path.abspath('.')\nEVAL_SCRIPT_PATH = os.path.join(USR_PATH, 'data_fusion_olci_dem.js') \n\n#delete download folder after processing - will save space but will download all the data for each requwst (otherwise the cached tile data will be used)\nDELETE_DOWNLOAD_FOLDER = True\n\n#OUTPUT_DIR = os.path.join(USR_PATH, \"output\") #local folder\nOUTPUT_DIR = \"/home/jovyan/result-data\" # will be copied to the local folder of the user requesting the data\n\nlogging.info(\"System configuration ok.\")    \n\n\n\n\n# initial ENV configuration\nSH_CLIENT_ID = %env SH_CLIENT_ID\nSH_CLIENT_SECRET = %env SH_CLIENT_SECRET\n\nsh_config = SHConfig()\nsh_config.sh_base_url = \"https://services.sentinel-hub.com\"\nsh_config.download_timeout_seconds=300\nsh_config.download_sleep_time=20\n\nsh_config.sh_client_id = SH_CLIENT_ID\nsh_config.sh_client_secret = SH_CLIENT_SECRET\n\nsh_config.save()\n\n# Evalscript\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [\n      {\n        datasource: \"OLCI\",\n        bands: [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B10\", \"B11\", \"B12\", \"B13\", \"B14\", \"B15\", \"B16\", \"B17\", \"B18\", \"B19\", \"B20\", \"B21\", \"SZA\", \"VZA\", \"SAA\", \"VAA\", \"TOTAL_COLUMN_OZONE\"],\n      },\n      {\n        datasource: \"COP_30\",\n        bands: [\"DEM\"]\n      }\n    ],\n    output: [\n      {\n        id: \"r_TOA_01\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"r_TOA_06\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"r_TOA_17\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"r_TOA_21\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"snow_grain_diameter\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"snow_specific_surface_area\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"diagnostic_retrieval\",\n        bands: 1,\n        sampleType: \"UINT8\",\n      },\n      {\n        id: \"albedo_bb_planar_sw\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      },\n      {\n        id: \"albedo_bb_spherical_sw\",\n        bands: 1,\n        sampleType: \"FLOAT32\",\n      }\n\n    ],\n    mosaicking: \"SIMPLE\",\n  };\n}\n\n//function updateOutput(outputs, collections) {\n//  Object.values(outputs).forEach((output) => {\n//    output.bands = collections.scenes.length;\n//  });\n//}\n\n// Set constants as global variables which can be used in all functions\nvar wls = {\n  \"B01\": 0.4000E+00,\n  \"B02\": 0.4125E+00,\n  \"B03\": 0.4425E+00,\n  \"B04\": 0.4900E+00,\n  \"B05\": 0.5100E+00,\n  \"B06\": 0.5600E+00,\n  \"B07\": 0.6200E+00,\n  \"B08\": 0.6650E+00,\n  \"B09\": 0.6737E+00,\n  \"B10\": 0.6812E+00,\n  \"B11\": 0.7088E+00,\n  \"B12\": 0.7538E+00,\n  \"B13\": 0.7613E+00,\n  \"B14\": 0.7644E+00,\n  \"B15\": 0.7675E+00,\n  \"B16\": 0.7788E+00,\n  \"B17\": 0.8650E+00,\n  \"B18\": 0.8850E+00,\n  \"B19\": 0.9000E+00,\n  \"B20\": 0.9400E+00,\n  \"B21\": 0.1020E+01\n};\n\nvar bai = {\n  \"B01\": 2.365E-11,\n  \"B02\": 2.7E-11,\n  \"B03\": 7.0E-11,\n  \"B04\": 4.17E-10,\n  \"B05\": 8.04E-10,\n  \"B06\": 2.84E-09,\n  \"B07\": 8.58E-09,\n  \"B08\": 1.78E-08,\n  \"B09\": 1.95E-08,\n  \"B10\": 2.1E-08,\n  \"B11\": 3.3E-08,\n  \"B12\": 6.23E-08,\n  \"B13\": 7.1E-08,\n  \"B14\": 7.68E-08,\n  \"B15\": 8.13E-08,\n  \"B16\": 9.88E-08,\n  \"B17\": 2.4E-07,\n  \"B18\": 3.64E-07,\n  \"B19\": 4.2E-07,\n  \"B20\": 5.53e-07,\n  \"B21\": 2.25E-06\n}\n\nlet emptyBandObject = {\n  \"B01\": {},\n  \"B02\": {},\n  \"B03\": {},\n  \"B04\": {},\n  \"B05\": {},\n  \"B06\": {},\n  \"B07\": {},\n  \"B08\": {},\n  \"B09\": {},\n  \"B10\": {},\n  \"B11\": {},\n  \"B12\": {},\n  \"B13\": {},\n  \"B14\": {},\n  \"B15\": {},\n  \"B16\": {},\n  \"B17\": {},\n  \"B18\": {},\n  \"B19\": {},\n  \"B20\": {},\n  \"B21\": {}\n}\n\nconst allBands = Object.keys(bai);\n\n// solar spectrum constants\nconst f0 = 32.38;\nconst f1 = -160140.33;\nconst f2 = 7959.53;\nconst bet = 1. / (85.34 * 1.e-3);\nconst gam = 1. / (401.79 * 1.e-3);\n\nvar coef1, coef2 = analyt_func(0.3, 0.7);\nvar coef3, coef4 = analyt_func(0.7, 0.865);\n\nfunction evaluatePixel(samples) {\n  const n_acquisitions = samples.OLCI.length;\n  const olciSamples = samples.OLCI;\n  const demSamples = samples.COP_30;\n  let r_TOA_01_valid = [];\n  let r_TOA_06_valid = [];\n  let r_TOA_17_valid = [];\n  let r_TOA_21_valid = [];\n  let snow_grain_diameter = [];\n  let snow_specific_surface_area = [];\n\n  // new products\n  let diagnostic_retrieval = [];\n  let albedo_bb_planar_sw = [];\n  let albedo_bb_spherical_sw = [];\n\n\n\n  // Loop through acquisition dates. No need to use this for single acquisition processing.\n  for (aq = 0; aq < n_acquisitions; aq++) {\n    // Correct TOA reflectance for ozone absorption\n    sample_cor_o3 = ozone_correction(olciSamples[aq]);\n\n    //Transfer of OLCI relative azimuthal angle to the definition used in radiative transfer code\n    angles = view_geometry(olciSamples[aq]);\n\n    //Filtering pixels unsuitable for retrieval\n    [snow, sample_fltr] = prepare_processing(sample_cor_o3, demSamples[0]);\n\n    //Compute snow properties\n    [sample_valid, angles_valid, snow] = snow_properties(sample_fltr, angles, snow);\n\n    //Compute aerosol\n    aerosol = aerosol_properties(demSamples[0].DEM, angles_valid.cos_sa);\n\n    //Compute atmosphere\n    atmosphere = prepare_coef(aerosol, angles);\n\n    //Compute theoretical reflectance of snow from albedo\n    snow = clean_snow_albedo(sample_valid, angles, aerosol, atmosphere, snow);\n\n    //throw new Error(JSON.stringify(snow))\n\n    snow = polluted_snow_albedo(sample_valid, angles, aerosol, atmosphere, snow);\n\n    snow = compute_plane_albedo(snow, angles);\n\n    r_TOA_01_valid.push(sample_valid.B01);\n    r_TOA_06_valid.push(sample_valid.B06);\n    r_TOA_17_valid.push(sample_valid.B17);\n    r_TOA_21_valid.push(sample_valid.B21);\n    snow_grain_diameter.push(snow.diameter);\n    snow_specific_surface_area.push(snow.area);\n    diagnostic_retrieval.push(snow.isnow);\n    albedo_bb_planar_sw.push(snow.rp3);\n    albedo_bb_spherical_sw.push(snow.rs3);\n\n    // Development use. Allow you to check the output before the script is finished.\n    //throw new Error(JSON.stringify(snow))\n  }\n  return {\n    r_TOA_01: r_TOA_01_valid,\n    r_TOA_06: r_TOA_06_valid,\n    r_TOA_17: r_TOA_17_valid,\n    r_TOA_21: r_TOA_21_valid,\n    snow_grain_diameter: snow_grain_diameter,\n    snow_specific_surface_area: snow_specific_surface_area,\n    diagnostic_retrieval: diagnostic_retrieval,\n    albedo_bb_planar_sw: albedo_bb_planar_sw,\n    albedo_bb_spherical_sw: albedo_bb_spherical_sw\n  };\n}\n\nfunction deg2rad(deg) {\n  const pi = Math.PI;\n  return deg * (pi / 180);\n}\n\nfunction ozone_correction(sample) {\n  const tozone_dict = {\n    \"B01\": 1.378170469E-004,\n    \"B02\": 3.048780958E-004,\n    \"B03\": 1.645714060E-003,\n    \"B04\": 8.935947110E-003,\n    \"B05\": 1.750535146E-002,\n    \"B06\": 4.347104369E-002,\n    \"B07\": 4.487130794E-002,\n    \"B08\": 2.101591797E-002,\n    \"B09\": 1.716230955E-002,\n    \"B10\": 1.466298300E-002,\n    \"B11\": 7.983028470E-003,\n    \"B12\": 3.879744653E-003,\n    \"B13\": 2.923775641E-003,\n    \"B14\": 2.792211429E-003,\n    \"B15\": 2.729651478E-003,\n    \"B16\": 3.255969698E-003,\n    \"B17\": 8.956858078E-004,\n    \"B18\": 5.188799343E-004,\n    \"B19\": 6.715773241E-004,\n    \"B20\": 3.127781417E-004,\n    \"B21\": 1.408798425E-005\n  };\n  let toa_cor_o3 = {};\n  // Loop through OLCI bands\n  allBands.forEach(band => {\n    todadu = sample.TOTAL_COLUMN_OZONE * 46696.24;\n    inv_cos_za = 1 / Math.cos(deg2rad(sample.SZA)) + 1 / Math.cos(deg2rad(sample.VZA));\n    toa_cor_o3[band] = sample[band] * 1 * Math.exp(inv_cos_za * tozone_dict[band] * todadu / 404.59);\n  });\n\n  toa_cor_o3.SZA = sample.SZA;\n  toa_cor_o3.VZA = sample.VZA;\n  toa_cor_o3.SAA = sample.SAA;\n  toa_cor_o3.VAA = sample.VAA;\n  return toa_cor_o3;\n}\n\nfunction view_geometry(sample) {\n  const raa = 180 - (sample.VAA - sample.SAA);\n  const sin_sza = Math.sin(deg2rad(sample.SZA));\n  const sin_vza = Math.sin(deg2rad(sample.VZA));\n  const cos_sza = Math.cos(deg2rad(sample.SZA));\n  const cos_vza = Math.cos(deg2rad(sample.VZA));\n  const ak1 = 3 * (1 + 2 * cos_sza) / 7;\n  const ak2 = 3 * (1 + 2 * cos_vza) / 7;\n  const cos_raa = Math.cos(deg2rad(raa));\n  const inv_cos_za = 1 / cos_sza + 1 / cos_vza;\n  const cos_sa = -cos_sza * cos_vza + sin_sza * sin_vza * cos_raa;\n  return {\n    raa: raa,\n    cos_sza: cos_sza,\n    cos_vza: cos_vza,\n    ak1: ak1,\n    ak2: ak2,\n    inv_cos_za: inv_cos_za,\n    cos_sa: cos_sa\n  };\n}\n\nfunction prepare_processing(sample_cor, sample_dem) {\n  let snow = {};\n  let sample_fltr = {};\n\n  // Assign diagnostic code\n  snow.isnow = sample_cor.B21 < 0.1 ? 102 : NaN;\n  snow.isnow = sample_cor.SZA > 75 ? 100 : snow.isnow;\n  const mask = isNaN(snow.isnow);\n\n  // Loop through OLCI bands\n  allBands.forEach(band => {\n    sample_fltr[band] = (mask == true) ? sample_cor[band] : NaN;\n  });\n\n  // Add elevation info to the object\n  sample_fltr.elevation = (mask == true) ? sample_dem.DEM : NaN;\n  return [\n    snow,\n    sample_fltr\n  ];\n}\n\nfunction snow_properties(sample_fltr, angles, snow) {\n  const akap2 = 2.25e-6;\n  const alpha2 = 4 * Math.PI * akap2 / 1.020;\n  const eps = 1.549559365010611;\n  const rr1 = sample_fltr.B17;\n  const rr2 = sample_fltr.B21;\n  const ak1 = angles.ak1;\n  const ak2 = angles.ak2;\n  const r0 = Math.pow(rr1, eps) * Math.pow(rr2, (1 - eps));\n  const bal = Math.pow((Math.log(rr2 / r0) / (ak1 * ak2 / r0)), 2) / alpha2;\n  const al = bal / 1000;\n  const D = al / (9.2 * 16 / 9);\n  const area = 6 / D / 0.917;\n\n  //Filtering small D\n  const diameter_thresh = 0.01;\n  const valid = D >= diameter_thresh;\n  snow.isnow = (!valid && isNaN(snow.isnow)) ? 104 : snow.isnow;\n\n  //Loop through toa bands\n  let sample_valid = {};\n\n  allBands.forEach(band => {\n    sample_valid[band] = valid ? sample_fltr[band] : NaN;\n  });\n\n  //Assign valid snow properties\n  snow.diameter = valid ? D : NaN;\n  snow.area = valid ? area : NaN;\n  snow.al = valid ? al : NaN;\n  snow.r0 = valid ? r0 : NaN;\n  snow.bal = valid ? bal : NaN;\n\n  //Loop through angles attributes\n  let angles_valid = {};\n  const angles_attrs = Object.keys(angles);\n  for (attr = 0; attr < angles_attrs.length; attr++) {\n    angles_valid[angles_attrs[attr]] = valid ? angles[angles_attrs[attr]] : NaN;\n  }\n  return [\n    sample_valid,\n    angles_valid,\n    snow\n  ];\n}\n\nfunction aerosol_properties(height, cos_sa, aot = 0.1) {\n  // Set an aerosol object to store the result. \n  let aerosol = {\n    \"tau\": {},\n    \"p\": {},\n    \"g\": {},\n    \"gaer\": {},\n    \"taumol\": {},\n    \"tauaer\": {}\n  };\n\n  // Loop through OLCI bands\n  allBands.forEach(band => {\n    tauaer = aot * Math.pow((wls[band] / 0.5), -1.3);\n    g0 = 0.5263;\n    g1 = 0.4627;\n    wave0 = 0.4685;\n    gaer = g0 + g1 * Math.exp(-wls[band] / wave0);\n    pr = 0.75 * (1 + Math.pow(cos_sa, 2));\n    taumol = Math.pow(wls[band], -4.05) * Math.min(1, Math.exp(-height / 7400)) * 0.00877;\n    tau = tauaer + taumol;\n    g = tauaer * gaer / tau;\n    pa = (1 - Math.pow(g, 2)) / Math.pow((1 - 2 * g * cos_sa + Math.pow(g, 2)), 1.5);\n    p = (taumol * pr + tauaer * pa) / tau;\n\n    // Assign results to objects. Values can be called via, e.g., aerosol.tau.B01\n    Object.assign(aerosol.tau, { [band]: tau })\n    Object.assign(aerosol.p, { [band]: p })\n    Object.assign(aerosol.g, { [band]: g })\n    Object.assign(aerosol.gaer, { [band]: gaer })\n    Object.assign(aerosol.taumol, { [band]: taumol })\n    Object.assign(aerosol.tauaer, { [band]: tauaer })\n  });\n  return aerosol;\n}\n\n// Solar flux\nfunction sol(x) {\n  // SOLAR SPECTRUM at GROUND level\n  // Inputs:\n  // x - wave length in micrometer\n  // Outputs: \n  // sol - solar spectrum in W m-2 micrometer-1 (?)\n  sol1a = f0 * x;\n  sol1b = - f1 * Math.exp(-bet * x) / bet;\n  sol1c = - f2 * Math.exp(-gam * x) / gam;\n  return sol1a + sol1b + sol1c;\n}\n\nfunction analyt_func(z1, z2) {\n  // see BBA_calc_pol\n  // compatible with array\n  var gam2 = Math.exp(gam, 2);\n  var gam3 = Math.exp(gam, 3);\n\n  var z12 = Math.exp(z1, 2);\n  var z13 = Math.exp(z1, 3);\n\n  var z22 = Math.exp(z2, 2);\n  var z23 = Math.exp(z2, 3);\n\n  var bet2 = Math.exp(bet, 2);\n  var bet3 = Math.exp(bet, 3);\n\n  var ak1 = (z22 - z12) / 2.0;\n  var ak2 = (z2 / bet + 1.0 / bet2) * Math.exp(-bet * z2) - (z1 / bet + 1.0 / bet2) * Math.exp(-bet * z1);\n  var ak3 = (z2 / gam + 1.0 / gam2) * Math.exp(-gam * z2) - (z1 / gam + 1.0 / gam2) * Math.exp(-gam * z1);\n  var am_minus = (z12 / bet + 2.0 * z1 / bet2 + 2.0 / bet3) * Math.exp(-bet * z1);\n\n  var am1 = (z23 - z13) / 3.0;\n  var am2 = (z22 / bet + 2.0 * z2 / bet2 + 2.0 / bet3) * Math.exp(-bet * z2) - am_minus\n  var am3 = (z22 / gam + 2.0 * z2 / gam2 + 2.0 / gam3) * Math.exp(-gam * z2) - am_minus;\n\n  return (f0 * ak1 - f1 * ak2 - f2 * ak3), (f0 * am1 - f1 * am2 - f2 * am3);\n}\n\nfunction prepare_coef(aerosol, angles) {\n  // Set an atmosphere object \n  let atmosphere = {\n    \"t1\": {},\n    \"t2\": {},\n    \"ratm\": {},\n    \"r\": {}\n  };\n\n  // Loop through OLCI bands\n  allBands.forEach(band => {\n    tau = aerosol.tau[band];\n    g = aerosol.g[band];\n    p = aerosol.p[band];\n    cos_sza = angles.cos_sza;\n    cos_vza = angles.cos_vza;\n    inv_cos_za = angles.inv_cos_za;\n\n    one_g_tau = (1 - g) * tau;\n\n    b1 = 1 + 1.5 * cos_sza + (1 - 1.5 * cos_sza) * Math.exp(-tau / cos_sza);\n    b2 = 1 + 1.5 * cos_vza + (1 - 1.5 * cos_vza) * Math.exp(-tau / cos_vza);\n\n    sumcos = cos_sza + cos_vza;\n\n    astra = (1 - Math.exp(-tau * inv_cos_za)) / sumcos / 4;\n    oskar = 4 + 3 * one_g_tau;\n\n    rms = 1 - b1 * b2 / oskar + (3 * (1 + g) * (cos_sza * cos_vza) - 2 * sumcos) * astra;\n\n    r = p * astra + rms;\n\n    wa1 = 1.10363;\n    wa2 = -6.70122;\n    wx0 = 2.19777;\n    wdx = 0.51656;\n    bex = Math.exp((g - wx0) / wdx);\n\n    arg = -0.5 * one_g_tau / ((wa1 - wa2) / (1 + bex) + wa2);\n\n    t1 = Math.exp(arg / cos_sza);\n    t2 = Math.exp(arg / cos_vza);\n\n    a_s = [.18016, -0.18229, 0.15535, -0.14223];\n    bs = [.58331, -0.50662, -0.09012, 0.0207];\n    cs = [0.21475, -0.1, 0.13639, -0.21948];\n    als = [0.16775, -0.06969, 0.08093, -0.08903];\n    bets = [1.09188, 0.08994, 0.49647, -0.75218];\n\n    a_cst = a_s[0] + a_s[1] * g;\n    b_cst = bs[0] + bs[1] * g;\n    c_cst = cs[0] + cs[1] * g;\n    al_cst = als[0] + als[1] * g;\n    bet_cst = bets[0] + bets[1] * g;\n\n    for (num = 2; num < 4; num++) {\n      if (num == 2) {\n        gg = Math.pow(g, 2);\n      } else {\n        gg *= g;\n      }\n      a_cst += a_s[num] * gg\n      b_cst += bs[num] * gg\n      c_cst += cs[num] * gg\n      al_cst += als[num] * gg\n      bet_cst += bets[num] * gg\n    }\n    ratm = tau * (a_cst * Math.exp(-tau / al_cst) + b_cst * Math.exp(-tau / bet_cst) + c_cst)\n\n    // Assign the results to the objects\n    Object.assign(atmosphere.t1, { [band]: t1 });\n    Object.assign(atmosphere.t2, { [band]: t2 });\n    Object.assign(atmosphere.ratm, { [band]: ratm });\n    Object.assign(atmosphere.r, { [band]: r });\n  });\n\n  return atmosphere;\n}\n\nfunction alb2rtoa(a, t1, t2, r0, ak1, ak2, ratm, r) {\n  const surf = t1 * t2 * r0 * Math.pow(a, (ak1 * ak2 / r0)) / (1 - a * ratm);\n  const rs = r + surf;\n  return rs;\n}\n\nfunction clean_snow_albedo(sample_valid, angles, aerosol, atmosphere, snow) {\n  snow.rs_1 = alb2rtoa(1, atmosphere.t1.B01, atmosphere.t2.B01, 1, 1, 1, atmosphere.ratm.B01, atmosphere.r.B01);\n  snow.ind_clean = sample_valid.B01 >= snow.rs_1;\n  snow.isnow = (snow.ind_clean) ? 0 : snow.isnow;\n  snow.ind_pol = sample_valid.B01 < snow.rs_1;\n\n  var alb_sph = Object.assign({}, emptyBandObject);\n  allBands.forEach(band => {\n    alb_sph[band] = Math.min(Math.exp(-Math.sqrt(1000 * 4 * Math.PI * (bai[band] / wls[band] * snow.al))), 1);\n  });\n\n  // Assign the results to the objects\n  snow.alb_sph = alb_sph;\n  return snow;\n}\n\nfunction polluted_snow_albedo(sample_valid, angles, aerosol, atmosphere, snow) {\n  //throw new Error(JSON.stringify(sample_valid));\n\n  if (snow.ind_pol) {\n    snow.isnow = (snow.ind_pol) ? 1 : snow.isnow;\n    ind_very_dark = (sample_valid.B21 < 0.4 && snow.ind_pol);\n    snow.isnow = (ind_very_dark) ? 6 : snow.isnow;\n    snow.r0 = (!ind_very_dark) ? snow.isnow : compute_rclean(angles.cos_sza, angles.cos_vza, angles.cos_sa, angles.raa);\n    //snow.alb_sph = (snow.ind_pol) ? 1 : snow.alb_sph; Guess it applies to all bands\n    Object.keys(snow.alb_sph).forEach(band => {\n      snow.alb_sph[band] = (snow.ind_pol) ? 1 : snow.alb_sph[band]\n    })\n\n    const bands_to_loop_over = Object.keys(sample_valid);\n    bands_to_loop_over.splice(18, 2);\n\n    //throw new Error(JSON.stringify(bands_to_loop_over));\n\n    for (i = 0; i < bands_to_loop_over.length; i++) {\n      var band = bands_to_loop_over[i];\n      snow.alb_sph[band] = (snow.ind_pol) ? solver_wrapper(sample_valid[band], atmosphere.t1[band], atmosphere.t2[band], snow.r0, angles.ak1, angles.ak2, atmosphere.ratm[band], atmosphere.r[band]) : snow.alb_sph[band];\n      snow.isnow = snow.alb_sph[band] == -999 ? -(i + 1) : snow.isnow;\n    }\n\n    // Guess it applies to all bands\n    Object.keys(snow.alb_sph).forEach(band => {\n      snow.alb_sph[band] = snow.isnow ? snow.alb_sph[band] : NaN;\n    })\n\n    let ind_clear_pol = (snow.alb_sph.B01 > 0.98 | snow.alb_sph.B02 > 0.98) & snow.ind_pol;\n    snow.isnow = ind_clear_pol ? 7 : snow.isnow;\n\n    // Guess it applies to all bands\n    if (!ind_clear_pol) {\n      allBands.forEach(band => {\n        snow.alb_sph[band] = Math.exp(-Math.sqrt(4 * 1000 * Math.PI * snow.al * (bai[band] / wls[band])));\n      });\n    }\n\n    snow.ind_pol = snow.ind_pol & (snow.isnow != 7);\n\n    // reprocessing of albedo to remove gaseous absorption using linear polynomial approximation in the range 753-778nm.\n    // Meaning: alb_sph[12],alb_sph[13] and alb_sph[14] are replaced by a linear  interpolation between alb_sph[11] and alb_sph[15]\n    if (ind_clear_pol) {\n      afirn = snow.alb_sph['B15'] - snow.alb_sph['B11'] / (wls['B15'] - wls['B11']);\n      bfirn = snow.alb_sph['B15'] - afirn * wls['B15'];\n\n      // indeces of bands start with 0!\n      allBands.concat().splice(11, 3).forEach(band => {\n        snow.alb_sph[band] = bfirn + afirn * wls[band];\n      });\n    }\n\n    // pixels that are clean enough in channels 18 19 20 and 21 are not affected by pollution, the analytical equation can then be used\n    ind_ok = (sample_valid.B20 > 0.35) & snow.ind_pol;\n    if (ind_ok) {\n      allBands.concat().splice(17, 4).forEach(band => {\n        snow.alb_sph[band] = Math.exp(-Math.sqrt(4. * 1000. * Math.PI * snow.al * (bai[band] / wls[band])));\n      });\n    }\n\n    //to avoid the influence of gaseous absorption (water vapor) we linearly interpolate in the range 885-1020nm for bare ice cases only (low toa[20])\n    //Meaning: alb_sph[18] and alb_sph[19] are replaced by a linear interpolation between alb_sph[17] and alb_sph[20]\n    if (ind_ok) {\n      bcoef = (snow.alb_sph['B20'] - snow.alb_sph['B17']) / (wls['B20'] - wls['17'])\n      acoef = snow.alb_sph['B20'] - bcoef * wls['B20']\n      allBands.concat().splice(17, 2).forEach(band => {\n        snow.alb_sph[band] = acoef + bcoef * wls[band];\n      });\n    }\n  }\n  return snow;\n}\n\nfunction compute_plane_albedo(snow, angles) {\n  var rp = Object.assign({}, emptyBandObject);\n  var refl = Object.assign({}, emptyBandObject);\n\n  snow.rp = rp;\n  snow.refl = refl;\n\n  allBands.forEach(band => {\n    snow.rp[band] = Math.pow(snow.alb_sph[band], angles.ak1);\n    snow.refl[band] = snow.r0[band] * Math.pow(snow.alb_sph[band], angles.ak1 * angles.ak2 / snow.r0);\n  });\n\n  ind_all_clean = snow.ind_clean || (snow.isnow == 7);\n\n\n  if (ind_all_clean) {\n    snow.rp3 = plane_albedo_sw_approx(snow.diameter, angles.cos_sza);\n    snow.rs3 = spher_albedo_sw_approx(snow.diameter);\n  }\n\n  // solar flux calculation\n  // sol1      visible(0.3 - 0.7micron)\n  // somehow, a different sol1 needs to be used for clean snow and polluted snow\n  var sol1_pol = sol(0.7) - sol(0.3);\n  // sol2      near - infrared(0.7 - 2.4micron)\n  // same for clean and polluted\n  var sol2 = sol(2.4) - sol(0.7);\n\n  // sol3      shortwave(0.3 - 2.4 micron)\n  // sol3 is also different for clean snow and polluted snow\n  var sol3_pol = sol1_pol + sol2;\n\n  // asol specific band\n  var asol = sol(0.865) - sol(0.7);\n\n  if (snow.ind_pol) {\n    snow.rp3 = BBA_calc_pol(snow.rp, asol, sol1_pol, sol3_pol);\n    snow.rs3 = BBA_calc_pol(snow.alb_sph, asol, sol1_pol, sol3_pol);\n  }\n\n  return snow;\n}\n\nfunction BBA_calc_pol(alb, asol, sol1_pol, sol3_pol) {\n  // polluted snow\n  // NEW CODE FOR BBA OF BARE ICE\n  // alb is either the planar or spherical albedo\n\n  // ANAlYTICal EQUATION FOR THE NOMINATOR\n  // integration over 3 segments\n\n  // segment 1\n  // QUADRATIC POLYNOMIal for the range 400 - 709nm\n  // input wavelength\n  //    alam2 = w[0]\n  //    alam3 = w[5]\n  //    alam5 = w[10]\n  //    alam6 = w[11]\n  //    alam7 = w[16]\n  //    alam8 = w[20]\n\n  const alam2 = 0.4;\n  const alam3 = 0.56;\n  const alam5 = 0.709;\n  const alam6 = 0.753;\n  const alam7 = 0.865;\n  const alam8 = 1.02;\n\n  // input reflectances\n  var r2 = alb['B01'];\n  var r3 = alb['B05'];\n  var r5 = alb['B10'];\n  var r6 = alb['B011'];\n  var r7 = alb['B16'];\n  var r8 = alb['B20'];\n\n  // declared outside\n  //var coef1, coef2 = analyt_func(0.3, 0.7);\n  //var coef3, coef4 = analyt_func(0.7, 0.865);\n\n  var a1, b1, c1 = quad_func(alam2, alam3, alam5, r2, r3, r5)\n  var aj1 = a1 * sol1_pol + b1 * coef1 + c1 * coef2;\n\n  // segment 2.1\n  // QUADRATIC POLYNOMIal for the range 709 - 865nm\n  var a2, b2, c2 = quad_func(alam5, alam6, alam7, r5, r6, r7)\n  var aj2 = a2 * asol + b2 * coef3 + c2 * coef4;    // segment 2.2\n\n  // exponential approximation for the range 865 - 2400 nm\n  const z1 = 0.865;\n  const z2 = 2.4;\n  var rati = r7 / r8;\n  var alasta = (alam8 - alam7) / Math.log(rati);\n  var an = 1. / alasta;\n  var p = r7 * Math.exp(alam7 / alasta);\n\n  var aj31 = (1. / an) * (Math.exp(-an * z2) - Math.exp(-an * z1));\n  var aj32 = (1. / (bet + an)) * (Math.exp(-(bet + an) * z2) - Math.exp(-(an + bet) * z1));\n  var aj33 = (1. / (gam + an)) * (Math.exp(-(gam + an) * z2) - Math.exp(-(an + gam) * z1));\n  var aj3 = (-f0 * aj31 - f1 * aj32 - f2 * aj33) * p;\n\n  return (aj1 + aj2 + aj3) / sol3_pol;\n}\n\nfunction quad_func(x0, x1, x2, y0, y1, y2) {\n  // quadratic function used for the polluted snow BBA calculation\n  // see BBA_calc_pol\n  // compatible with arrays\n  var d1 = (x0 - x1) * (x0 - x2);\n  var d2 = (x1 - x0) * (x1 - x2);\n  var d3 = (x2 - x0) * (x2 - x1);\n\n  var a1 = x1 * x2 * y0 / d1 + x0 * x2 * y1 / d2 + x0 * x1 * y2 / d3;\n  var b1 = -(x1 + x2) * y0 / d1 - (x0 + x2) * y1 / d2 - (x0 + x1) * y2 / d3;\n  var c1 = y0 / d1 + y1 / d2 + y2 / d3;\n  var x = x1;\n  return a1, b1, c1;\n}\n\nfunction spher_albedo_sw_approx(D) {\n  return 0.6420 + 0.1044 * Math.exp(-1000 * D / 158.62) + 0.1773 * Math.exp(-1000 * D / 2448.18);\n}\n\nfunction plane_albedo_sw_approx(D, cos_sza) {\n  var cos_sza2 = Math.exp(cos_sza, 2);\n  var anka = 0.7389 - 0.1783 * cos_sza + 0.0484 * cos_sza2;\n  var banka = 0.0853 + 0.0414 * cos_sza - 0.0127 * cos_sza2\n  var canka = 0.1384 + 0.0762 * cos_sza - 0.0268 * cos_sza2;\n  var diam1 = 187.89 - 69.2636 * cos_sza + 40.4821 * cos_sza2;\n  var diam2 = 2687.25 - 405.09 * cos_sza + 94.5 * cos_sza2;\n  return anka + banka * Math.exp(-1000 * D / diam1) + canka * Math.exp(-1000 * D / diam2)\n}\n\nfunction compute_rclean(cos_sza, cos_vza, cos_sa, raa) {\n  const am11 = Math.sqrt(1 - Math.pow(cos_sza, 2));\n  const am12 = Math.sqrt(1 - Math.pow(cos_vza, 2));\n  const theta = Math.acos(-cos_sza * cos_vza + am11 * am12 * Math.cos(raa * 3.14159 / 180)) * 180 / Math.PI;\n  const pz = 11.1 * Math.exp(-0.087 * theta) + 1.1 * Math.exp(-0.014 * theta);\n  const sumcos = cos_sza + cos_vza;\n  const rclean = 1.247 + 1.186 * sumcos + 5.157 * cos_sza * cos_vza + pz;\n  return rclean / 4 / sumcos;\n}\n\nfunction solver_wrapper(toa_cor_o3, t1, t2, r0, ak1, ak2, ratm, r) {\n  return zbrent(0.1, 1, args = [t1, t2, r0, ak1, ak2, ratm, r, toa_cor_o3], max_iter = 30, tolerance = 2e-4);\n}\n\nfunction zbrent(x0, x1, args, max_iter = 100, tolerance = 1e-6) {\n  let fx0 = f(x0, ...args = args);\n  let fx1 = f(x1, ...args = args);\n  if ((fx0 * fx1) > 0) {\n    return -999;\n  }\n  if (Math.abs(fx0) < Math.abs(fx1)) {\n    [x0, x1] = [x1, x0];\n    [fx0, fx1] = [fx1, fx0];\n  }\n  let [x2, fx2] = [x0, fx0];\n  let d = x2;\n  let mflag = true;\n  let steps_taken = 0;\n  while (steps_taken < max_iter && Math.abs(x1 - x0) > tolerance) {\n    fx0 = f(x0, ...args = args);\n    fx1 = f(x1, ...args = args);\n    fx2 = f(x2, ...args = args);\n\n    if (fx0 != fx2 && fx1 != fx2) {\n      let L0 = (x0 * fx1 * fx2) / ((fx0 - fx1) * (fx0 - fx2));\n      let L1 = (x1 * fx0 * fx2) / ((fx1 - fx0) * (fx1 - fx2));\n      let L2 = (x2 * fx1 * fx0) / ((fx2 - fx0) * (fx2 - fx1));\n      var new_value = L0 + L1 + L2;\n    } else {\n      var new_value = x1 - ((fx1 * (x1 - x0)) / (fx1 - fx0));\n    }\n\n    if (\n      (new_value < ((3 * x0 + x1) / 4) || new_value > x1) ||\n      (mflag == true && (Math.abs(new_value - x1)) >= (Math.abs(x1 - x2) / 2)) ||\n      (mflag == false && (Math.abs(new_value - x1)) >= (Math.abs(x2 - d) / 2)) ||\n      (mflag == true && (Math.abs(x1 - x2)) < tolerance) ||\n      (mflag == false && (Math.abs(x2 - d)) < tolerance)\n    ) {\n      new_value = (x0 + x1) / 2;\n      mflag = true;\n    } else {\n      mflag = false;\n    }\n\n    let fnew = f(new_value, ...args = args);\n    [d, x2] = [x2, x1];\n\n    if (fx0 * fnew < 0) {\n      x1 = new_value;\n    } else {\n      x0 = new_value;\n    }\n\n    if (Math.abs(fx0) < Math.abs(fx1)) {\n      [x0, x1] = [x1, x0];\n    }\n\n    steps_taken += 1;\n  }\n  return x1;\n}\n\n// not used?\nfunction f(albedo, t1, t2, r0, ak1, ak2, ratm, r, toa_cor_o3) {\n  const surf = t1 * t2 * r0 * albedo ** (ak1 * ak2 / r0) / (1 - albedo * ratm);\n  const rs = r + surf\n  return toa_cor_o3 - rs;\n}\n\"\"\"\n\nlogging.info(\"Configuration ok.\")    \n\n\n\ndate_range = (f'{date}T8:00:00', f'{date}T18:00:00')\n\nDATE_FOLDER = date.replace(\"-\",\"_\")\nDL_FOLDER =  os.path.join('downloads', str(resolution), DATE_FOLDER)\nPROCESSED_FOLDER = f'{DL_FOLDER}/processed'\n\nDATE_FOLDER\n\n\n\naoi_gpd = gpd.GeoDataFrame(geometry=[wkt.loads(aoi)], crs = \"EPSG:4326\")\n\n\n\n# load the 1 degree world grid from which the tiles will be calculated\nlogging.info(\"Loading grid\")\ngrid = gpd.read_file(os.environ[\"DATA_PATH\"] + \"/wgs84-1degree.geojson\")\n\n\n\nlogging.info(\"Calculating tiles to be processed.\")    \n\n# make the intersection with the polygon so that we only calculate tiles inside the polygon\ntoProcess=gpd.overlay(aoi_gpd, grid, how='intersection')\n\n#Remove chunks that are too small to be processed\nMIN_AREA = 1e-1\ntoProcess = toProcess[toProcess.geometry.to_crs('epsg:4326').area > MIN_AREA]\n\n\n\n# use full frames\ntoProcess = grid[grid.ID.isin(list(toProcess.ID.values))]\n\n\n\nif DELETE_DOWNLOAD_FOLDER and os.path.exists(DL_FOLDER):\n    shutil.rmtree(DL_FOLDER)\n    logging.info(f'Folder {DL_FOLDER} deleted')\n\n\n\nlistBbox = [BBox(bbox=geom, crs=\"EPSG:4326\") for geom in toProcess.geometry]\nlistSizes = [bbox_to_dimensions(bbox, resolution=resolution) for bbox in listBbox]\nfolderPaths = [f'{DL_FOLDER}/{round(id)}' for id in toProcess.ID]\n\n\n\nrequests = [SentinelHubRequest(\n            evalscript=evalscript,\n            data_folder=folderPath,\n            input_data=[\n                SentinelHubRequest.input_data(\n                    data_collection=DataCollection.DEM_COPERNICUS_30,\n                    identifier=\"COP_30\",\n                    upsampling=\"NEAREST\",\n                    downsampling=\"NEAREST\",\n                ),\n                SentinelHubRequest.input_data(\n                    data_collection=DataCollection.SENTINEL3_OLCI,\n                    identifier=\"OLCI\",\n                    time_interval=date_range,\n                    upsampling=\"NEAREST\",\n                    downsampling=\"NEAREST\",\n                ),\n            ],\n            responses=[\n                SentinelHubRequest.output_response('r_TOA_01', MimeType.TIFF),\n                SentinelHubRequest.output_response('r_TOA_06', MimeType.TIFF),\n                SentinelHubRequest.output_response('r_TOA_17', MimeType.TIFF),\n                SentinelHubRequest.output_response('r_TOA_21', MimeType.TIFF),\n                SentinelHubRequest.output_response('snow_grain_diameter', MimeType.TIFF),\n                SentinelHubRequest.output_response('snow_specific_surface_area', MimeType.TIFF),\n                SentinelHubRequest.output_response('diagnostic_retrieval', MimeType.TIFF),\n                SentinelHubRequest.output_response('albedo_bb_planar_sw', MimeType.TIFF),\n                SentinelHubRequest.output_response('albedo_bb_spherical_sw', MimeType.TIFF),\n            ],\n            bbox=bb,\n            size=size,\n            config=sh_config\n        ) for bb, size, folderPath in zip(listBbox, listSizes, folderPaths)]\n\n#extract only downloader\nlist_of_requests = [request.download_list[0] for request in requests]\n\n\n\nlogging.info('Starting download')\ndownloader = SentinelHubDownloadClient(config=sh_config)\ndownloader.download(list_of_requests, max_threads=20, show_progress=False)\nlogging.info('Done')\n\n\n\ndef unzipFile(file, destination):\n    if not os.path.exists(os.path.join(destination, 'snow_grain_diameter.tif')):\n        tar = tarfile.open(file, \"r:\")\n        tar.extractall(path=destination)\n        tar.close()\n\n\n\nclass ConcurrentUnzipper:\n    def __init__(self, files, destinations):\n        self.files = files\n        self.destinations = destinations\n        \n    def operation(self, chunk):\n        unzipFile(self.files[chunk] , self.destinations[chunk])\n        \n    def unzipAll(self):\n        with concurrent.futures.ProcessPoolExecutor() as executor:\n            list(executor.map(self.operation, np.arange(0, len(self.files)), chunksize=1))\n\n\n\nfilenamesList = glob.glob(f'./{DL_FOLDER}/*/*/response.tar')\ndestinations =  [file.replace('response.tar', '') for file in filenamesList]        \n        \nlogging.info('Extracting .tar responses')\nunzipper = ConcurrentUnzipper(filenamesList, destinations)\nunzipper.unzipAll()\nlogging.info('Done')\n\n\n\ndef mergeProduct(productName):\n    prodResult = productName.replace(\".tif\", '_merged.tif')\n    if os.path.exists(prodResult):\n        os.remove(prodResult)\n        logging.info(f'Deleted file: {prodResult}')\n    filenamesList = glob.glob(f'./{DL_FOLDER}/*/*/{productName}')\n    merge_tiffs(filenamesList, prodResult, overwrite=True)\n     \nclass TifMerger:\n    def __init__(self, products):\n        self.products = products\n        \n    def operation(self, chunk):\n        mergeProduct(self.products[chunk])\n        \n    def process(self):\n        with concurrent.futures.ProcessPoolExecutor() as executor:\n            list(executor.map(self.operation, np.arange(0, len(self.products)), chunksize=1))    \n\n\n\n#define the products that will be computed\nproducts = ['diagnostic_retrieval.tif','albedo_bb_planar_sw.tif','albedo_bb_spherical_sw.tif','snow_grain_diameter.tif', 'snow_specific_surface_area.tif', 'r_TOA_01.tif', 'r_TOA_06.tif', 'r_TOA_17.tif', 'r_TOA_21.tif']\n#products = ['snow_grain_diameter.tif']\n\n#merge individual tiles into one single image\nlogging.info('Merging products')\nunzipper = TifMerger(products)\nunzipper.process()\nlogging.info('Done')    \n\nresultsDataPath = os.path.join(USR_PATH, PROCESSED_FOLDER)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif not os.path.exists(PROCESSED_FOLDER):\n    os.makedirs(PROCESSED_FOLDER)\n    \nproducedFiles = glob.glob('*_merged.tif')\nfor prodResult in producedFiles:\n    shutil.move(prodResult, f'{PROCESSED_FOLDER}/{prodResult}')\n    \nresultsDataPath = os.path.join(USR_PATH, PROCESSED_FOLDER)\nresultsDataPath\n\n\n\nprocessedDataPath = os.path.join(USR_PATH, PROCESSED_FOLDER)\nif len(projection) > 0 and not projection == '4326' : \n    destFolder = f'{DL_FOLDER}/warped/'\n\n    logging.info('Reprojecting to epsg:{projection}')\n    \n    if os.path.exists(destFolder):\n        shutil.rmtree(destFolder)\n\n    os.makedirs(destFolder)\n\n    for product in products:\n        srcFile = processedDataPath + \"/\" + product.replace(\".tif\", \"_merged.tif\")\n        destFile = f'{destFolder}/{product}'\n        cmd = f'gdalwarp {srcFile} {destFile}  -t_srs EPSG:{projection}'\n        os.system(cmd)\n   \n    resultsDataPath = destFolder\nelse:\n    for product in products:\n        srcFile = processedDataPath + \"/\" + product.replace(\".tif\", \"_merged.tif\")\n        destFile = processedDataPath + \"/\" + product\n        if os.path.exists(srcFile):\n            os.rename(srcFile, destFile)\n\n","type":"content","url":"/notebooks/sice","position":1},{"hierarchy":{"lvl1":"Ocean Acidification"},"type":"lvl1","url":"/notebooks/ccadi-uc3","position":0},{"hierarchy":{"lvl1":"Ocean Acidification"},"content":"!pip3 install altair===4.2.2\n!pip3 install PyCO2SYS==1.8.2\n!pip3 install leafmap==0.17.1\n!pip3 install erddapy===1.2.1\n!pip3 install ipywidgets==8.0.4\n\n\n\nimport urllib.request as request\nimport h5py\nimport os\nfrom ipywidgets import widgets, IntSlider, jslink, interact, interactive, fixed, interact_manual\nimport markdown\nfrom erddapy import ERDDAP\nfrom ipyleaflet import Map, Marker, GeoData, ImageOverlay, basemaps, basemap_to_tiles, LayersControl, ScaleControl, FullScreenControl, WidgetControl\n# import pandas as pd\nimport numpy as np\n# from IPython.display import display\nfrom netCDF4 import num2date\n# from datetime import datetime\nimport geopandas as gpd\n########################################################\nimport sys\nimport pandas as pd\nimport ipywidgets as widgets\nfrom IPython.display import Markdown, HTML, Javascript, display, Image\nimport subprocess\nimport csv\nfrom __future__ import print_function\n# from ipywidgets import interact, interactive, fixed, interact_manual\nimport csv\nimport re\nimport warnings\n#from init import *\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\ndef printmd(string):\n    display(Markdown(string))\n###############################################################\nfrom ftplib import FTP\nimport multiprocessing as mlp\nimport shutil\nimport tempfile\nimport urllib.request\nfrom datetime import datetime, timedelta\n\nfrom osgeo import gdal\n# import numpy as np\nfrom osgeo import ogr\nfrom osgeo import osr\n# import pandas as pd\nimport pyproj\n\nfrom multiprocessing import Manager\n# from ipywidgets import widgets, IntSlider, jslink\nfrom ipyleaflet import Map, projections, GeoData, basemap_to_tiles, basemaps, WidgetControl, ScaleControl, FullScreenControl, LayersControl #, ImageOverlay, \nimport geopandas as gpd\nimport leafmap\nimport altair as alt\n# import localtileserver  # was needed localy to be able to add raster on the leafmap Map.\n\n\n\nclass merge_btl_nutrient:\n    \n    def get_btlfile(self, btl_url, wdir):\n        request.urlretrieve(btl_url, os.path.join(wdir, \"btl.h5\"))\n        return 0\n\n    def btl_to_dataframe(self, wdir):\n        f = h5py.File(os.path.join(wdir, \"btl.h5\"), 'r')\n        df_btl = pd.DataFrame()\n        for grp in f:\n            tempo_list = []\n            tempo_columns = []\n            for c in f[grp]:\n                tempo_columns.append(c)\n                tempo_list.append(f[grp][c])\n            list_array = np.transpose(np.array(tempo_list))\n            tempo_df = pd.DataFrame(list_array, columns=tempo_columns)\n            tempo_df['station'] = [f[grp].attrs['Station'].strip().split(' ')[-1]] * len(tempo_df)\n            tempo_df['cast'] = [int(f[grp].attrs['Cast_Number'].strip())] * len(tempo_df)\n            df_btl = pd.DataFrame.append(tempo_df, df_btl)\n        f.close()\n#         # added to extract a csv format of the btl.h5 data to send out to the GUI team\n#         df_btl.to_csv(os.path.join(wdir, \"btl.csv\"), header=1, index=0)\n        #######\n        return df_btl\n\n    def merge(self, df_nutrient, df_btl, file_ge_btl):\n        ge_time_header = [i for i in list(df_nutrient.columns) if\n                          ((i.lower().__contains__('date')) or \n                           (i.lower().__contains__('time')))].pop()\n        btl_time_header = [i for i in list(df_btl.columns) if\n                           ((i.lower().__contains__('date')) or \n                           (i.lower().__contains__('time')))].pop()\n        ge_station_header = [i for i in list(df_nutrient.columns) if\n                             i.lower().__contains__('station')].pop()\n        btl_station_header = [i for i in list(df_btl.columns) if\n                              i.lower().__contains__('station')].pop()\n        ge_bottle_header = [i for i in list(df_nutrient.columns) if\n                            (i.lower().__contains__('bottle'))].pop()\n        \"\"\" TODO: bopo should be replacet with RosPos when the data with the corrected variable name will be served on Hyrax.\"\"\"\n        btl_bottle_header = [i for i in list(df_btl.columns) if\n                             i.lower().__contains__('bopo')].pop() \n        ge_cast_header = [i for i in list(df_nutrient.columns) if\n                          i.lower().__contains__('cast')].pop()\n        btl_cast_header = [i for i in list(df_btl.columns) if\n                           i.lower().__contains__('cast')].pop()\n        ge_jointField = [ge_time_header, ge_station_header, ge_bottle_header, ge_cast_header]\n        btl_jointField = [btl_time_header, btl_station_header, btl_bottle_header, btl_cast_header]\n        \n        #####################################\n        df_nutrient_header = [h.split(' ')[0] for h in df_nutrient.columns]\n        df_nutrient.columns = df_nutrient_header\n        dfnutrient_to_merge = df_nutrient\n        dfbtl_to_merge = df_btl\n        dfbtl_to_merge = dfbtl_to_merge.drop(btl_time_header, axis=1)\n        dfnutrient_to_merge = dfnutrient_to_merge.drop(ge_time_header, axis=1)\n        dfnutrient_to_merge = dfnutrient_to_merge.drop(ge_station_header, axis=1)\n        dfnutrient_to_merge[ge_time_header] = pd.to_datetime(df_nutrient[ge_time_header]).dt.strftime('%Y-%m-%d')\n        # dfnutrient_to_merge[ge_time_header] = df_nutrient[ge_time_header].dt.strftime('%Y-%m-%d')\n        u = []\n        for i in df_nutrient[ge_station_header].values:\n            if i.isdigit():\n                u.append('G' + i)\n            else:\n                u.append(i)\n        dfnutrient_to_merge[ge_station_header] = u\n        hdf_time_units = \"days since 1970-01-01 00:00:00\"\n        list_tmp = []\n        import cftime\n        for i in range(len(df_btl[btl_time_header])):\n            u=num2date(df_btl[btl_time_header].values[i], hdf_time_units)\n            u=cftime.DatetimeGregorian.strftime(u, '%Y-%m-%d')\n            list_tmp.append(u)\n        dfbtl_to_merge[btl_time_header] = list_tmp\n        '''https://www.datasciencemadesimple.com/join-merge-data-frames-pandas-python/'''\n        df = pd.merge(dfnutrient_to_merge, dfbtl_to_merge, how=\"inner\", left_on=ge_jointField, right_on=btl_jointField)\n        df.to_csv(file_ge_btl, header=1, index=0)\n        del df\n        return 0\n\n\nclass merging_gui_jupiter():\n    # This is the class where the GUI is made\n    def __init__(self):\n        self.gridwindow={} # making an empty grid window\n        self.vbox_widgets = [] # making an empty vertical box\n        self.gridwindow['grid'] = widgets.GridspecLayout(1, 1)\n        #####\n        self.getBTLbutton = widgets.Button(description=\"retrieve\", layout=widgets.Layout(width='max-content'), button_style='info')\n        self.getNutrientbutton = widgets.Button(description=\"retrieve\", layout=widgets.Layout(width='max-content'), button_style='info')\n        self.Continuebutton = widgets.Button(description=\"Continue\", layout=widgets.Layout(width='max-content'))\n        # BTL file retrieval\n        layout = widgets.Layout(height='auto', width='125px')\n        printmd('<h1><b>Merging bottle file with the nutrient file</b></h1>')\n        # read text\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/nutrient_btl_infos.md\",\"r\")\n        fc=f.read()\n        f.close()\n        text_html1 = markdown.markdown(fc)\n        del fc\n        self.gridwindow['text1'] = widgets.HTML(text_html1)\n#         # Reading the images of the CTD-Rosette ##########\n        ctd_img = open(os.environ[\"DATA_PATH\"] + \"/images/ctd-rosette.jpg\", \"rb\")\n        ship_img = open(os.environ[\"DATA_PATH\"] + \"/images/CCGSAmundsen.png\", 'rb')\n        ctd = ctd_img.read()\n        ship = ship_img.read()\n        gridimage1 = widgets.Image(value=ship, format='jpg', width=300)\n        Figure1 = widgets.Label(r'\\(\\textbf{Figure 1:}\\)'+' Canadian Coast Guard Ship ' + r'\\(\\textit{Amundsen}\\)', layout=widgets.Layout(height='auto', width='auto'))\n        gridimage2 = widgets.Image(value=ctd, format='png', width=300)\n        Figure2 = widgets.Label(r'\\(\\textbf{Figure 2:}\\)'+' CTD-Rosette', layout=widgets.Layout(height='auto', width='auto'))\n        ship_img.close()\n        ctd_img.close()\n        image_vbox1 = widgets.VBox(children=[gridimage1, Figure1])\n        image_vbox2 = widgets.VBox(children=[gridimage2, Figure2])\n        self.gridwindow['image'] = widgets.HBox(children=[image_vbox1, image_vbox2])\n# #         image_vbox = [[gridimage1, gridimage2], ['CCGS Amundsen', 'CTD-Rosette']]\n#         gridwindow['image'] = widgets.HBox(children=[gridimage2, gridimage1])\n        ###################################################\n        self.vbox_widgets.append(self.gridwindow['text1'])\n        self.vbox_widgets.append(self.gridwindow['image'])\n        \n                # read text\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/data_retrieval.md\",\"r\")\n        fc=f.read()\n        f.close()\n        data_retrieval = markdown.markdown(fc)\n        del fc\n        self.gridwindow['data_retrieval'] = widgets.HTML(data_retrieval)\n        self.vbox_widgets.append(self.gridwindow['data_retrieval'])\n        ## Bottle file retrieval ######\n        label = widgets.Label('Bottle files', layout=layout)\n        self.BottleData = widgets.Text(\n            value=\"http://jorvik.uwaterloo.ca:8080/opendap/data/CCADI/Amundsen_BTL_GreenEdge2016_LEG1.h5\",\n            layout=widgets.Layout(width='50%')\n        )\n        self.gridwindow['bottle'] = widgets.HBox(children=[label, self.BottleData, self.getBTLbutton])\n        self.vbox_widgets.append(self.gridwindow['bottle'])\n        label = widgets.Label('Nutrient file', layout=layout)\n        self.nutrientServer = widgets.Text(\n            value=\"https://CanWINerddap.ad.umanitoba.ca/erddap\",\n            layout=widgets.Layout(width='50%')\n        )\n        self.gridwindow['nutrientserver'] = widgets.HBox(children=[label, self.nutrientServer, self.getNutrientbutton])\n        self.vbox_widgets.append(self.gridwindow['nutrientserver'])\n        \n        self.list0 = widgets.SelectMultiple(\n            options=[\"Empty\"],\n            value=[\"Empty\"],\n            disabled=False\n        )\n\n        self.list1 = widgets.SelectMultiple(\n            options=[\"Empty\"],\n            value=[\"Empty\"],\n            disabled=False\n        )\n        \n        self.depthRange = widgets.FloatRangeSlider(\n                value=[0, 0],\n                min=0,\n                max=5000,\n                step=0.1,\n                disabled=False,\n                continuous_update=False,\n                orientation='horizontal',\n                readout=True,\n                readout_format='.1f',\n        )\n        \n        self.outputdir = \"2016_int_btl_csv\"\n        self.gridwindow['grid'][0, 0] = widgets.VBox(children=self.vbox_widgets)  # pass all the content of the vertical box into the left side of the grid\n        \n        self.getBTLbutton.on_click(self.getBTLdata)\n        self.getNutrientbutton.on_click(self.getNutrientdata)\n        display(self.gridwindow['grid'])\n        self.Merge_Button=widgets.Button(\n            description='Merge',\n            disabled=False,\n            button_style='', \n            tooltip='Click me',\n            icon=''\n            )\n        self.merge_btl_nutrient()\n        \n        \n        ###########\n             \n    def continue_to_pyco2sys(self): \n        def on_button_pyco2sys(b):\n            continueprocess()\n\n        outmerge=widgets.Output()\n        @outmerge.capture()\n        def continueprocess():\n            checkInputfile()\n#             self.continue_to_sic()\n        \n        gridwindow={}\n        vbox_widgets = []\n        gridwindow['grid'] = widgets.GridspecLayout(1,1)\n        layout = widgets.Layout(height='auto', width='125px')\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/variable_Join_list.md\",\"r\")\n        fc=f.read()\n        f.close()\n        text_var_sel = markdown.markdown(fc)\n        del fc\n        gridwindow['variable_selection'] = widgets.HTML(text_var_sel)\n        vbox_widgets.append(gridwindow['variable_selection'])\n        label = widgets.Label('Bottle_variables', layout=widgets.Layout(width='50%'))\n        label = widgets.Label('Nutrient variables', layout=widgets.Layout(width='50%'))\n        gridwindow['bottle variable list'] = widgets.HBox(children=[label, self.list0])\n\n        gridwindow['nutrient variable list'] = widgets.HBox(children=[label, self.list1])\n        gridwindow['var_list'] = widgets.HBox(children=[gridwindow['bottle variable list'], gridwindow['nutrient variable list']])\n        vbox_widgets.append(gridwindow['var_list'])\n        label = widgets.Label('Sample depth:', layout=layout)\n        # read text\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/variable_meaning.md\",\"r\")\n        fc=f.read()\n        f.close()\n        text_var = markdown.markdown(fc)\n        del fc\n        gridwindow['variable_meaning'] = widgets.HTML(text_var)\n        # read text\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/sample_depth_range.md\",\"r\")\n        fc=f.read()\n        f.close()\n        text_var_sel = markdown.markdown(fc)\n        del fc\n        gridwindow['sample_depth_range'] = widgets.HTML(text_var_sel)\n        vbox_widgets.append(gridwindow['sample_depth_range'])\n        gridwindow['Sample depth'] = widgets.HBox(children=[label, self.depthRange])\n        vbox_widgets.append(gridwindow['Sample depth'])\n\n        self.btl = pd.DataFrame()\n        self.nutrient=pd.DataFrame()\n        \n        continue_button1=widgets.Button(\n            description='Continue',\n            disabled=False,\n            button_style='', \n            tooltip='Click me',\n            icon=''\n            )\n        gridwindow['merge'] = widgets.HBox(children=[self.Merge_Button])\n        gridwindow['to_pyco2sys'] = widgets.HBox(children=[gridwindow['merge'], continue_button1])\n        vbox_widgets.append(gridwindow['to_pyco2sys'])\n        gridwindow['grid'][0, 0] = widgets.VBox(children=vbox_widgets)\n        self.Merge_Button.on_click(self.clickMerge)\n        continue_button1.on_click(on_button_pyco2sys)\n        display(gridwindow['grid'])\n        display(outmerge)\n        return 0       \n\n\n        \n    def merge_btl_nutrient(self): \n        def on_button_continuemerge(b):\n            continuemerge()\n\n        out=widgets.Output()\n        @out.capture()\n        def continuemerge():           \n            ###### transit to the PyCO2SYS #####\n            self.continue_to_pyco2sys()\n            \n\n        self.continue_button=widgets.Button(\n            description='continue',\n            disabled=False,\n            button_style='', \n            tooltip='Click me',\n            icon=''\n            )\n        self.gridwindow['continue'] = widgets.HBox(children=[self.continue_button])\n        self.vbox_widgets.append(self.gridwindow['continue'])\n        self.gridwindow['grid'][0, 0] = widgets.VBox(children=self.vbox_widgets)\n        self.continue_button.on_click(on_button_continuemerge)\n        display(out)\n\n    def getBTLdata(self,a):\n        if not os.path.exists(self.outputdir):\n            os.makedirs(self.outputdir)\n        merge_btl_nutrient().get_btlfile(self.BottleData.value, self.outputdir)\n        df_btl=merge_btl_nutrient().btl_to_dataframe(self.outputdir)\n        self.list0.options=tuple(df_btl.columns)\n        self.list0.value=[\"BOPO\",\"CTDTmp90\",\"Cast_Number\",\"P_sal_CTD\",\"Pres_Z\",\"depth\",\"latitude\",\"longitude\",\"time\",\"station\"]\n        self.list0.rows = 24\n        self.depthRange.min = df_btl['depth'].min()\n        self.depthRange.max = df_btl['depth'].max()\n        self.depthRange.value = [self.depthRange.min, self.depthRange.max]\n        self.getBTLbutton.description=\"Success!\"\n        self.getBTLbutton.button_style='success'\n        del df_btl\n        return 0\n    \n    def getNutrientdata(self, a):\n        if not os.path.exists(self.outputdir):\n            os.makedirs(self.outputdir)\n        e_DataSearch = ERDDAP(server=self.nutrientServer.value)\n        result_search = e_DataSearch.get_search_url(search_for=\"greenedge\", response=\"csv\")\n        self.datasetID = [k \n                          for k in pd.read_csv(result_search)[\"Dataset ID\"] \n                          if k.lower().__contains__(\"greenedge_nutrient\")].pop()\n        #self.datasetID = pd.read_csv(result_search)[\"Dataset ID\"][0]\n        #print(result_search)\n\n        e_datafetch = ERDDAP(server=self.nutrientServer.value, protocol=\"tabledap\", response=\"csv\")\n        e_datafetch.dataset_id = self.datasetID\n\n        df_nutrient = e_datafetch.to_pandas(parse_dates=True)\n        file_ge = os.path.join(self.outputdir, f'{self.datasetID}.csv')  ## Nutrient file name \n        df_nutrient.to_csv(file_ge, index=False, header=True)\n        self.list1.options=df_nutrient.columns\n        \"\"\" Adjustment done in order to look easily for the variables needed in the data field\"\"\"\n        station = [k for k in df_nutrient.columns if k.lower().__contains__(\"station\")].pop()\n        sample_date = [k for k in df_nutrient.columns if k.lower().__contains__(\"sample_date\")].pop()\n        #sample_date = sample_date.split(\" \")[0]\n        sample_depth = [k for k in df_nutrient.columns if k.lower().__contains__(\"sample_depth\")].pop()\n        #sample_depth = sample_depth.split(\" \")[0]\n        cast = [k for k in df_nutrient.columns if k.lower().__contains__(\"cast\")].pop()\n        bottle = [k for k in df_nutrient.columns if k.lower().__contains__(\"bottle\")].pop()\n        dic_um = [k for k in df_nutrient.columns if k.lower().__contains__(\"dic_um\")].pop()\n        #dic_um = dic_um.split(\" \")[0]\n        totalk = [k for k in df_nutrient.columns if k.lower().__contains__(\"totalk_l_um\")].pop()\n        #totalk = totalk.split(\" \")[0]\n        self.list1.value=[station, sample_date, sample_depth, cast, bottle,dic_um, totalk]\n        self.list1.rows = 24\n        self.getNutrientbutton.description=\"Success!\"\n        self.getNutrientbutton.button_style='success'\n        del df_nutrient\n        return 0\n\n        \n    def clickMerge(self, a):\n        file_ge_btl = os.path.join(self.outputdir, 'merged_btl_nutrient.csv') ## Merged file name to be fed to the PyCO2SYS\n        objectsForMerging = merge_btl_nutrient()\n        if os.path.exists(file_ge_btl):\n            os.remove(file_ge_btl)\n            df_btl = objectsForMerging.btl_to_dataframe(self.outputdir)\n            df_btl.reset_index(drop=True, inplace=True)\n            cond = (df_btl[\"depth\"].values[:]>=self.depthRange.value[0]) & (df_btl[\"depth\"].values[:]<=self.depthRange.value[1])\n            df_btl = df_btl.loc[cond]\n            df_nutrient = pd.read_csv(os.path.join(self.outputdir, f'{self.datasetID}.csv'), header=0)            \n            objectsForMerging.merge(df_nutrient=df_nutrient[list(self.list1.value)], df_btl=df_btl[list(self.list0.value)], file_ge_btl=file_ge_btl)\n            del df_nutrient, df_btl\n        else:\n            df_btl = objectsForMerging.btl_to_dataframe(self.outputdir)\n            df_btl.reset_index(drop=True, inplace=True)\n            cond = (df_btl[\"depth\"].values[:]>=self.depthRange.value[0]) & (df_btl[\"depth\"].values[:]<=self.depthRange.value[1])\n            df_btl = df_btl.loc[cond]\n            df_nutrient = pd.read_csv(os.path.join(self.outputdir, f'{self.datasetID}.csv'), header=0)            \n            objectsForMerging.merge(df_nutrient=df_nutrient[list(self.list1.value)], df_btl=df_btl[list(self.list0.value)], file_ge_btl=file_ge_btl)\n            del df_nutrient, df_btl\n        self.Merge_Button.description=\"Done\"\n        self.Merge_Button.button_style=\"success\"\n        return 0\n\n\n\n\n\n#Define the required parameters\n\ndef checkInputfile():\n    #Create an empty list for all the required parameters in the file. \n    req_param_inFile=[]\n    opt_param_inFile=[]\n    \n    pd.set_option('display.max_columns', 50)\n    \n    #-------------------INPUT FILE-------------------------------------------\n    input_file =os.path.join(\"2016_int_btl_csv\", \"merged_btl_nutrient.csv\")                \n    df = pd.read_csv(input_file)\n    df=df.reset_index(drop=True)\n    #------------------------------------------------------------------------\n    \n    #Checking for the stadardized names in the input file to automatically pull out all the required, optional and mandatory parameters present\n    \n    # 1. KEY PARAMETERS \n    # Parameters- Total Alkalinity, DIC, PH, PCO2, fCO2, CO232, biCO2\n    \n    standardizedNames=['TotAlk_l_um_l','DIC_um_l','pH','pCO2','fCO2','CO232','biCO2'] #BODC standardized names\n    fullNames=['Total alkalinity (umolkg1)','Dissolved inorganic carbon (umolkg1)','Partial pressure of carbon dioxide (pCO2) (uatm)',\\\n               'Fugacity of carbon dioxide (fCo2) (uatm)','Carbonate ion concentration (CO32) (umolkg1)','Bicarbonate ion (umol kg1)']   # Full name that will show up in widget\n    \n    \n    for name, fname in zip(standardizedNames, fullNames): \n        if name in df.columns:\n            req_param_inFile.append(fname) #Append the names of all the key parameters in the input file\n        \n    \n    # 2. OPTIONAL PARAMETERS \n    # Parameters- SiOx, PO4, Ammonia, Sulfide\n    \n    standardizedNames=['SiOx_um_l','PO4_Filt_um_l','Ammonia','Sulfide'] #BODC standardized names\n    fullNames=['Total Silicate (umolkg1)','Total Phosphate (SRP) (umolkg1)','Total Ammonia (umolkg1)','Total Sulfide (umolkg1)']   # Full name that will show up in widget\n    \n    for name, fname in zip(standardizedNames, fullNames): \n        if name in df.columns:\n            opt_param_inFile.append(fname) #Append the names of all the key parameters in the input file\n    \n    \n    getUserParameters(df, req_param_inFile, opt_param_inFile) \n\n\n\n\ndef getUserParameters(df,req_param_inFile,opt_param_inFile):\n\n    #The user will be presented with the parameters automatically pulled out from the input file. They will have a chance to make changes to the selections. \n    #Only exception is if there are only two req parameters in input file, they will not be able to make any changes/selctions in this case.\n    \n    #Key Parameters Widget\n    req_param_user=widgets.SelectMultiple(\n        options=req_param_inFile,\n        #value=req_param_inFile,\n        #description='Key Parameters:',\n        disabled=False,\n    )\n    req_param_user.layout.margin='0.5% 0% 5% 0%'\n    req_param_user.layout.width='20%'\n    req_param_user.layout.height='70%'\n\n\n    #Optional Parameters Widget\n    opt_param_user=widgets.SelectMultiple(\n        options=opt_param_inFile,\n        value=opt_param_inFile,\n        #description='Optional Parameters:',\n        disabled=False,\n    )\n    opt_param_user.layout.margin='0.5% 0% 3% 0%'\n    opt_param_user.layout.width='20%'\n\n    cont_button1=widgets.Button(\n    description='Continue',\n    disabled=False,\n    button_style='', \n    tooltip='Click me',\n    icon=''\n    )\n\n    \n    # Onclick function for the first Continue button widget\n    output = widgets.Output()\n    @output.capture()\n    def on_button_clicked(b):\n        getConstants(df, req_param_user, opt_param_user, req_param_inFile, opt_param_inFile)\n    \n\n    # Key parameters, aka carbonate system parameters\n    printmd('### <br><br/> Carbonate System Parameters ###') \n    printmd('More information on these arguments an be found [here](https://pyco2sys.readthedocs.io/en/latest/co2sys_nd/#carbonate-system-parameters).')\n    #if there is only one or no key parameters in input file\n    if len(req_param_inFile)<2:\n        printmd(\"<br>**There are not enough key parameters for calculation of the full carbonate system. Please check input file and try again.**<br />\")\n        sys.exit(-1)\n    \n    #If only two key parameters in the input file, automatically use those two\n    if len(req_param_inFile)==2:\n        printmd(\"<br>**The following key carbonate parameters were found in the input file and will be used in calculations.**<br />\")\n\n        for name in req_param_inFile:\n            printmd('- {}'.format(name))\n \n    #If there are more than two key parameters in the input file, ask user to select any two\n    if len(req_param_inFile)>2:\n        printmd(\"<br>**The following key carbonate parameters were found in the input file. Choose any two parameters.**<br />\")\n        display(req_param_user) #display widget\n\n\n    # Optional parameters, aka Nutrients & solutes\n    printmd('### <br><br/> Nutrients and other solutes ###') \n    printmd('More information on these arguments an be found [here](https://pyco2sys.readthedocs.io/en/latest/co2sys_nd/#nutrients-and-other-solutes).')\n    \n    # If there is at least one opt parameter in file, display them and ask user to select any of them. All are automatically selected in the widget\n    if len(opt_param_inFile)>0:\n        printmd(\"<br>**The following nutrient parameters are in the input file. Choose any parameter(s).**</b>\")\n        display(opt_param_user)   #display widget\n    \n    \n    display(cont_button1) #display continue button\n    cont_button1.on_click(on_button_clicked)  #Call onclick function\n    display(output) #display widget ouput when button is clicked\n    \n\n\n\ndef getConstants(df, req_param_user, opt_param_user, req_param_inFile, opt_param_inFile):    \n    \n    # Constants\n    printmd('### <br><br/> Settings for constants ###') \n    printmd('More information on these constants an be found [here](https://pyco2sys.readthedocs.io/en/latest/co2sys_nd/#settings). Default constants chosen based on [Jiang et al., 2022](https://www.frontiersin.org/articles/10.3389/fmars.2021.705638/full).')\n    #Widgets for the different constants\n    phstr = widgets.Output()\n    @phstr.capture()\n    def constStrings1():\n        printmd(\"<br>**Choose the pH scale:**\")\n        \n    constStrings1()\n\n    option_list=['1. Total',\n                 '2. Seawater',\n                 '3. Free',\n                 '4. NBS, i.e. relative to NBS/NIST reference standards']\n    \n    #PH Scale\n    phscale=widgets.RadioButtons(\n        options=option_list,  \n        disabled=False,\n        layout={'width': 'max-content'},\n    )\n    phscale.layout.margin='0.5% 1% 3% 0%'\n    #phscale.layout.width='40%' \n\n\n    # Carbonic Acid Dissociation\n    k1k2str = widgets.Output()\n    @k1k2str.capture()\n    def constStrings2():\n        printmd(\"**Choose the set of equilibrium constant parameterisations to model carbonic acid dissociation:**\")\n    constStrings2()\n\n    option_list=['1. RRV93 (0 < T < 45 C, 5 < S < 45, Total scale, artificial seawater).',\n                '2. GP89 (1 < T < 40 C, 10 < S < 50, Seawater scale, artificial seawater).',\n                '3. H73a and H73b refit by DM87 (2 < T < 35 C, 20 < S < 40, Seawater scale, artificial seawater).',\n                '4. MCHP73 refit by DM87 (2 < T < 35 C, 20 < S < 40, Seawater scale, real seawater).',\n                '5. H73a, H73b and MCHP73 refit by DM87(2 < T < 35 C, 20 < S < 40, Seawater scale, real seawater)',\n                '6. MCHP73 aka \"GEOSECS\" (2 < T < 35 C, 19 < S < 43, NBS scale, real seawater).',\n                '7. MCHP73 without certain species aka \"Peng\" (2 < T < 35 C, 19 < S < 43, NBS scale, real seawater).',\n                '8. M79 (0 < T < 50 C, S = 0, freshwater only).',            \n                '9. CW98 (2 < T < 30 C, 0 < S < 40, NBS scale, real estuarine seawater).',\n                '10. LDK00 (2 < T < 35 C, 19 < S < 43, Total scale, real seawater).',\n                '11. MM02 (0 < T < 45 C, 5 < S < 42, Seawater scale, real seawater).',\n                '12. MPL02 (1.6 < T < 35 C, 34 < S < 37, Seawater scale, field measurements).',\n                '13. MGH06 (0 < T < 50 C, 1 < S < 50, Seawater scale, real seawater).',\n                '14. M10 (0 < T < 50 C, 1 < S < 50, Seawater scale, real seawater).',\n                '15. WMW14 (0 < T < 45 C, 0 < S < 45, Seawater scale, real seawater).',\n                '16. SLH20 (1.67 < T < 31.80 C, 30.73 < S < 37.57, Total scale, field measurements).',\n                '17. SB21 (15 < T < 35 C, 19.6 < S < 41, Total scale, real seawater).']\n    \n    k1k2=widgets.RadioButtons(\n        options=option_list,    \n        value='10. LDK00 (2 < T < 35 C, 19 < S < 43, Total scale, real seawater).',\n        #rows=len(option_list),\n        layout={'width': 'max-content'},\n        disabled=False,\n    )\n    k1k2.layout.margin='0.5% 1% 3% 0%'\n    #k1k2.layout.width='50%'\n    #k1k2.layout.height='100%'\n\n    # Bisulfate ion dissociation \n    kso4str = widgets.Output()\n    @kso4str.capture()\n    def constStrings3():\n        printmd(\"**Choose the equilibrium constant parameterisations to model bisulfate ion dissociation:**\")\n    constStrings3()\n\n    option_list=['1. D90a: Dickson (1990) J. Chem. Thermodyn.',\n                '2. KRCB77: Khoo et al. (1977) Anal. Chem.',\n                '3. WM13: Waters & Millero (2013) Mar. Chem./ WMW14: Waters et al. (2014) Mar. Chem.']\n    \n    kso4=widgets.RadioButtons(\n        options=option_list,    \n        value='1. D90a: Dickson (1990) J. Chem. Thermodyn.',\n        layout={'width': 'max-content'},\n        #description='Parameter:',\n        disabled=False,\n    )\n    kso4.layout.margin='0.5% 1% 3% 0%'\n    #kso4.layout.width='40%'\n\n    # Total borate \n    bostr = widgets.Output()\n    @bostr.capture()\n    def constStrings4():\n        printmd(\"**Choose which boron:salinity relationship to use to estimate total borate:**\")\n    constStrings4()\n\n    option_list=['1. U74: Uppstrm (1974) DeepSea Res.',\n                '2. LKB10: Lee et al. (2010) Geochim. Cosmochim. Acta']\n    \n    bo=widgets.RadioButtons(\n        options=option_list,    \n        value='2. LKB10: Lee et al. (2010) Geochim. Cosmochim. Acta',\n        layout={'width': 'max-content'},\n        disabled=False,\n    )\n    bo.layout.margin='0.5% 1% 3% 0%'\n    # bo.layout.width='40%'\n    \n    # hydrogen fluoride dissociation\n    hfstr = widgets.Output()\n    @hfstr.capture()\n    def constStrings5():\n        printmd(\"**Choose which which equilibrium constant parameterisation to use for hydrogen fluoride dissociation:**\")\n    constStrings5()\n\n    option_list=['1. DR79: Dickson & Riley (1979) Mar. Chem.',\n                '2. PF87: Perez & Fraga (1987) Mar. Chem.']\n    \n    hf=widgets.RadioButtons(\n        options=option_list,    \n        value='2. PF87: Perez & Fraga (1987) Mar. Chem.',\n        layout={'width': 'max-content'},\n        disabled=False,\n    )\n    hf.layout.margin='0.5% 1% 3% 0%'\n    # hf.layout.width='40%' \n\n    # opt_gas_constant\n    gcstr = widgets.Output()\n    @gcstr.capture()\n    def constStrings6():\n        printmd(\"**Choose which value to use for the gas constant:**\")\n    constStrings6()\n    option_list=['1. DOEv2',\n                '2. DOEv3',\n                '3. 2018 CODATA']\n    \n    gc=widgets.RadioButtons(\n        options=option_list,    \n        value='3. 2018 CODATA',\n        layout={'width': 'max-content'},\n        disabled=False,\n    )\n    gc.layout.margin='0.5% 1% 3% 0%'\n    # gc.layout.width='40%' \n    \n    #Continue button On-click function\n    output = widgets.Output()\n    @output.capture()\n    def on_button_clicked(b):        \n        runPyco2sys(df,req_param_user, opt_param_user, req_param_inFile, opt_param_inFile, phscale, k1k2, kso4, bo, hf,gc)\n\n    # Button widget\n    button2=widgets.Button(\n    description='Continue',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    tooltip='Click me',\n    icon=''\n    )\n\n    box2 = widgets.VBox([phstr,phscale, k1k2str, k1k2, kso4str, kso4, bostr, bo, hfstr, hf, gcstr,gc, button2])\n    display(box2)\n\n    button2.on_click(on_button_clicked)\n    display(output)\n\n\n\ndef runPyco2sys(df,req_param_user, opt_param_user, req_param_inFile, opt_param_inFile, phscale, k1k2, kso4, bo, hf,gc):\n\n\n    #Build the argument list for pyco2sys to ingest\n    #The argument names are defined in the PyCO2 sys documentation.\n    kwargs={}\n   \n\n    #KEY PARAMETERS\n    \n    # pyco2sys labels the two key parameters chosen as par 1 and par 2. 'par1check' checks if par1 has yet been assigned, otherwise it sets a key parameter to par 2\n    par1check=\"False\"\n    par2check=\"False\"\n\n    \n    #Check if the parameter was chosen by the user, or if it was automatically selected (only two parameters were in the file). In that case the variable would not be in 'req_param_user' from the widget.\n    #If the varibale is to be used in the calculation, then get the data from the input file (saved as the data frame df)\n    #The variable is then added to the arguments for pyco2sys\n    \n    substrings=['alkalinity','Dissolved inorganic carbon','pH','Partial pressure of carbon dioxide', 'Fugacity of carbon dioxide','Carbonate ion concentration', 'Bicarbonate Ion']\n    standardizedNames=['TotAlk_l_um_l','DIC_um_l','pH','pCO2','fCO2','CO232','biCO2']\n    types=[1,2,3,4,5,6,7]   #These are the different types according to pyco2sys documentation \n    \n    \n    for name, substr, t in zip(standardizedNames, substrings, types):           # Loop through the substrings and the standardized names  \n        if par1check==\"False\" or par2check==\"False\":                            #Check if either par1 or par2 is false. We need at least two key paramaters (par1 and par2 represent these two parameters)\n            if any(substr in string for string in req_param_inFile):            #If it is in the input file\n                if len(req_param_inFile)==2:                                    #If it is one of only two key variables in the input file (here it would not be in 'req_param_user.value' as it would be automatically selected- no widget used)\n   \n                    if par1check==\"True\":        \n                        kwargs['par2']=df[name].to_numpy(dtype=float)            #Get the data using the standardized name from the data frame (input file)\n                        kwargs['par2_type']=t\n                        par2check=\"True\"\n                    else:\n                        kwargs['par1']=df[name].to_numpy(dtype=float)             \n                        kwargs['par1_type']=t\n                        par1check=\"True\"\n\n                elif len(req_param_inFile)>2:                                    # If there are more than two key parameters in the input file\n                    if any(substr in string for string in req_param_user.value): #If there are more than two key variables in the file, then check if the user actually selected this varibale\n                        if par1check==\"True\":        \n                            kwargs['par2']=df[name].to_numpy(dtype=float)          \n                            kwargs['par2_type']=t\n                            par2check=\"True\"\n                        else:\n                            kwargs['par1']=df[name].to_numpy(dtype=float)          \n                            kwargs['par1_type']=t\n                            par1check=\"True\"\n\n\n\n                            \n    # OPTIONAL PARAMETERS\n    \n    substrings_opt=['Silicate','Phosphate','Ammonia','Sulfide' ]\n    standardizedNames=['SiOx_um_l','PO4_Filt_um_l','Ammonia','Sulfide']\n    pyco2sysNames=['total_silicate','total_phosphate','total_phosphate','total_sulfide']  # Names that pyco2sys expects in the argument list\n    \n    \n    if len(opt_param_inFile)>0:  # if there is at least one optional paramter in the input file.        \n        for name, substr, pName in zip(standardizedNames, substrings_opt, pyco2sysNames):     \n            if any(substr in string for string in opt_param_user.value):  # If the optional parameter was chosen by the user\n                kwargs[pName]=df[name].to_numpy(dtype=float)              # Get the data using the standardized name\n        \n\n        \n    # MANDATORY  PARAMETERS\n\n    # Also check if the user wanted to use any other output temperature and pressure in the calculations.\n    # If they did not, the temp_out and press_out would be 'nan'\n    \n    #Set the output temperature and pressure to nan\n    Temperature_out=float('nan')\n    Pressure_out=float('nan')\n    \n    Temperature=df['CTDTmp90'].to_numpy(dtype=float)\n    kwargs['temperature']=Temperature\n    #if np.isnan(Temperature_out)==False:    \n    kwargs['temperature_out']=Temperature_out\n\n    Pressure=df['Pres_Z'].to_numpy(dtype=float)\n    kwargs['pressure']=Pressure   \n    #if np.isnan(Pressure_out)==False:    \n    kwargs['temperature_out']=Pressure_out   \n\n    Salinity=df['P_sal_CTD'].to_numpy(dtype=float)\n    kwargs['salinity']=Salinity   \n\n\n\n    #Get the values of the widget selections for the different CONSTANTS. All parameters are selected by default.\n    \n    k1k2Value=k1k2.value\n    kso4Value=kso4.value\n    boValue=bo.value\n    hfValue=hf.value\n    phscaleValue=phscale.value\n    gcValue=gc.value\n\n    kso4Value_temp=kso4Value[:2]\n    boValue_temp=boValue[:2]\n    hfValue_temp=hfValue[:2]\n    k1k2Value_temp=k1k2Value[:2]\n    phscaleValue_temp=phscaleValue[:2]\n    gcValue_temp=gcValue[:2]\n        \n    if '.' in kso4Value_temp:      \n        kso4Value_temp=kso4Value_temp[:1]\n    \n    if '.' in boValue_temp:       \n        boValue_temp=boValue_temp[:1]       \n\n    if '.' in hfValue_temp:       \n        hfValue_temp=hfValue_temp[:1]       \n        \n    if '.' in k1k2Value_temp:       \n        k1k2Value_temp=k1k2Value_temp[:1]\n              \n    if '.' in phscaleValue_temp:   \n        phscaleValue_temp=phscaleValue_temp[:1]\n        \n    if '.' in gcValue_temp:\n        gcValue_temp=gcValue_temp[:1]\n    \n    k1k2Value=int(k1k2Value_temp)\n    kso4Value=int(kso4Value_temp)\n    boValue=int(boValue_temp)\n    hfValue=int(hfValue_temp)\n    phscaleValue=int(phscaleValue_temp)\n    gcValue=int(gcValue_temp)\n        \n    #Add them as arguments for pyco2sys\n    kwargs['opt_k_carbonic']=k1k2Value\n    kwargs['opt_k_bisulfate']=kso4Value\n    kwargs['opt_total_borate']=boValue\n    kwargs['opt_k_fluoride']=hfValue\n    kwargs['opt_gas_constant']=gcValue\n\n\n    # Import PyCO2SYS\n    import PyCO2SYS as pyco2\n\n    # Run pyCO2SYS!\n    output_dict = pyco2.sys(**kwargs)\n\n    \n    Output(df,output_dict)\n\n\n\ndef Output(df,output_dict):\n    \n#     ## Added to make the \"continue button\" to continue the process into the Sea Ice Concentration retrieval: \n    def on_button_sic(b):\n        continue_sic()\n\n    outsic=widgets.Output()\n    @outsic.capture()\n    def continue_sic():\n        ccadi_uc3_mapping() # sea ice concentration processing\n\n    ###########################################\n    \n    \n    #The dictionary output_dict is uneven. Some elements are just a single int, string or float. The length of 'par1' will always be the longest length (it holds the first of the two key parameters from the input file)\n    #For the values in the dictionary that are not single values, they are arrays. \n    #To be able to create even data frames, we need to seperate the values in the array and create a list.\n    #Otherise, the array is saved as one value for each key, instad of a list of multiple values.\n    \n    \n    longlength=len(output_dict['par1'])\n    newlist=[]\n\n    for val in output_dict.values():\n\n        #if value is a single integer\"\n        if isinstance(val, int):\n            list0=[val] * longlength\n            newlist.append(list0)\n\n        elif isinstance(val, str):\n            list1=[val] * longlength\n            newlist.append(list1)\n\n        elif isinstance(val, float):\n            list2=[val] * longlength\n            newlist.append(list2)\n        else:\n            arr=val\n            list3 = arr.tolist()\n            newlist.append(list3)\n            \n            \n    #Creae a new dict that has keys associated with a list of values, all of the same length.            \n    newdict={}\n    i=0\n    for key in output_dict.keys():\n        newdict[key]=newlist[i]\n        i=i+1\n\n\n    #Create a new dataframe and save as csv.\n    output_df=pd.DataFrame.from_dict(newdict)\n\n    #Merge this resulting data frame with extra variables from the input file that were not used in calculations\n    cols_to_use = df.columns.difference(output_df.columns)   #variales that are different from those in output file\n    input_subset=df[cols_to_use]\n    merged_df = pd.concat([output_df,input_subset] , axis=1)  #Merged dataframe\n\n        \n    # Organize data frame so that specific varibales are at the front\n    front_metadata=['project name','platform name','Cruise', 'Station','sample date','TIME','latitude','longitude','sample depth','Cast','Bottle']\n    \n    #Loop through the list of metadata variables that should be at the front\n    col_position=-1\n    for var in front_metadata: \n        for col in merged_df.columns:                                  # Loop through all the columns in dataframe\n            stripped_string = re.sub(\"[^0-9a-zA-Z]+\", \" \",col)         # Strip the column headers of all non- laphanumeric characters\n            if var.lower() in stripped_string.lower():                 # Check for column name regardless of case\n                popped_col_data=merged_df.pop(col)                     # Pop the column from daat frame\n                col_position=col_position+1                            # Find the next front position\n                merged_df.insert(col_position, col, popped_col_data)   # Place variable at position\n\n    front_data=['saturation_aragonite', 'saturation_aragonite_out','saturation_calcite','saturation_calcite_out','pCO2','fCO2','bicarbonate','pH_total']\n\n    #Loop through the list of calculated carbonate chemistry variables that should be at the front\n    for var in front_data: \n        for col in merged_df.columns:                                  # Loop through all the columns in dataframe\n            if col==var:                 \n                popped_col_data=merged_df.pop(col)                     # Pop the column from daat frame\n                col_position=col_position+1                            # Find the next front position\n                merged_df.insert(col_position, col, popped_col_data)   # Place variable at position\n    \n    \n    #Remove par1, par2, par1 and par2 types- added by pyco2, not needed by user \n    merged_df.pop('par1')\n    merged_df.pop('par2')\n    merged_df.pop('par1_type')\n    merged_df.pop('par2_type')\n\n    #Remove duplicate columns from final data frame\n    duplicateColumnNames = list()\n\n    for x in range(merged_df.shape[1]):                   # Iterate over all the columns in dataframe\n        col_name1= merged_df.columns[x]                   # Select column at xth index.\n\n        for y in range(x + 1, merged_df.shape[1]):        # Iterate over all the columns in DataFrame from (x+1)th index till end\n            col_name2= merged_df.columns[y]\n            \n            if col_name1.lower()==col_name2.lower():      # Check if column names are the same regardless of case\n                duplicateColumnNames.append(col_name1)\n                continue\n                \n            if '.1' in col_name1:                         # Check if there is a duplicate (same case), pandas will save this with a .1 at the end of the duplicated variable\n                col_name1_stripped=col_name1.strip('.1')  # Remove .1 and check again for equality \n                if col_name1_stripped.lower()==col_name2.lower():\n                    duplicateColumnNames.append(col_name1)\n                    continue\n                \n            if '.1' in col_name2:\n                col_name2_stripped=col_name2.strip('.1')\n                if col_name1.lower()==col_name2_stripped.lower():\n                     duplicateColumnNames.append(col_name2)\n                        \n    merged_df = merged_df.drop(columns=duplicateColumnNames)  #Drop all duplicates\n    merged_df=merged_df.dropna(axis=1,how='all')              #Drop all empty columns\n\n    # OUTPUT FILE----------------------------------------------------------------------\n    if os.path.isfile(os.path.join(\"2016_int_btl_csv\", \"merged_btl_nutrient_pyco2sys.csv\"))==True:  \n        os.remove(os.path.join(\"2016_int_btl_csv\", \"merged_btl_nutrient_pyco2sys.csv\"))\n    merged_df.to_csv(os.path.join(\"2016_int_btl_csv\", \"merged_btl_nutrient_pyco2sys.csv\"), index=False)   \n    # OUTPUT FILE----------------------------------------------------------------------\n\n    \n    printmd('**<br />PCO2sys ran successfully! Output file is saved as merged_btl_nutrient_pyco2sys.csv**')\n    printmd(\"<br />**Retrieving ice concentration now...**\")\n \n    # add the \"continue button\" on the GUI ######################################\n    gridwindow={}\n    vbox_widgets = []\n    gridwindow['grid'] = widgets.GridspecLayout(1,1)\n\n    continue_button2=widgets.Button(\n        description='Continue',\n        disabled=False,\n        button_style='', \n        tooltip='Click me',\n        icon=''\n        )\n    gridwindow['to_sic'] =  widgets.HBox(children=[continue_button2])\n    vbox_widgets.append(gridwindow['to_sic'])\n    gridwindow['grid'][0, 0] = widgets.VBox(children=vbox_widgets)\n    display(gridwindow['grid'])\n    continue_button2.on_click(on_button_sic)\n    display(outsic)\n    ##################################################\n\n    \n\n\n\nL = mlp.Lock()\nclass addSeaIceConcentration:\n    def __init__(self, inputfile):\n        self.inputfile = inputfile\n\n    def createShapefile(self, df, output_shp, time_header, prglabel):\n        ''' This function creates a shapefile from the geographical locations inside the input CSV file.\n            The coordinate system used for the output shapetile is the World Geodetic System (WGS) 1984.\n            Both Latitude and Longitude are in decimal degrees'''\n        prglabel.value = 'Creating the final shapefile...'\n        shpfile = os.path.join(os.path.dirname(self.inputfile), output_shp, output_shp + '.shp')\n        if not os.path.exists(os.path.join(os.path.dirname(self.inputfile), output_shp)):\n            os.makedirs(os.path.join(os.path.dirname(self.inputfile), output_shp))\n        driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n        if os.path.exists(shpfile):\n            driver.DeleteDataSource(shpfile)\n        ds = driver.CreateDataSource(shpfile)\n        spref = osr.SpatialReference()\n        spref.ImportFromEPSG(4326)\n        layer = ds.CreateLayer('StationsLocations', spref, ogr.wkbPoint)\n        # create field to the layer\n        for c in df.columns:\n            u = list(df[c])\n            vint = [i for i in u if isinstance(i, int)]\n            vfloat = [i for i in u if isinstance(i, float)]\n            if c.__contains__(time_header):\n                layer.CreateField(ogr.FieldDefn(time_header, ogr.OFTDateTime))\n            elif c.__contains__('CIS_dates'):\n                layer.CreateField(ogr.FieldDefn('CIS_dates', ogr.OFTDateTime))\n            elif c.__contains__('Time'):\n                fieldname = ogr.FieldDefn('Time UTC', ogr.OFTString)\n                fieldname.SetWidth(20)\n                layer.CreateField(fieldname)\n            elif len(u) == len(vint):\n                df[c] = df[c].astype(float)\n                ##############################################\n                fieldname = ogr.FieldDefn(c, ogr.OFTInteger)\n                fieldname.SetPrecision(0)\n                layer.CreateField(fieldname)\n            elif len(u) == len(vfloat):\n                fieldname = ogr.FieldDefn(c, ogr.OFTReal)\n                fieldname.SetPrecision(6)\n                layer.CreateField(fieldname)\n            else:\n                df[c].astype(str)\n                fieldname = ogr.FieldDefn(c, ogr.OFTString)\n                fieldname.SetWidth(30)\n                layer.CreateField(fieldname)\n\n        c_fid = 0\n        ld = layer.GetLayerDefn()\n        lon = [i for i in df.columns if i.__contains__('longitude')].pop()\n        lat = [i for i in df.columns if i.__contains__('latitude')].pop()\n        for i in df.index:\n            # create new point object\n            point = ogr.Geometry(ogr.wkbPoint)\n            point.AddPoint(float(df[lon].values[i]), float(df[lat].values[i]))\n            # create new feature\n            featureDfn = layer.GetLayerDefn()\n            feature = ogr.Feature(featureDfn)\n            feature.SetGeometry(point)\n            for k in range(0, df.columns.__len__()):\n                fieldName = ld.GetFieldDefn(k).GetName()\n                feature.SetField(fieldName, df[df.columns[k]].values[i])\n            c_fid += 1\n            # add the new feature to the new layer\n            layer.CreateFeature(feature)\n        del layer, ds, df\n        return 0\n\n    def extractFromUniBremenAMSR2(self, prm):\n        ''' Sea Ice Concentration (SIC) from Bremen University are downloaded. The SIC located at each geographical\n        coordinate indicated in the input csv file are extracted and then added as an extra-column to the csv file. '''\n        L.acquire()\n        ddate = pd.to_datetime(prm[0])\n        dlat = prm[1]\n        dlon = prm[2]\n        q = prm[3]\n        m = datetime.strftime(ddate, '%b').lower()  # extract the month in lowercase charachter\n        y = datetime.strftime(ddate, '%Y')  # extract the year\n        with tempfile.TemporaryDirectory() as tmpDir:\n            url = r'https://seaice.uni-bremen.de/data/amsr2/asi_daygrid_swath/n6250/{0}/{1}/Arctic/asi-AMSR2-n6250-{2}-v5.4.tif'.format(\n                y, m, datetime.strftime(ddate, '%Y%m%d'))\n            if not os.path.exists(os.path.join(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters'),\n                                               'asi-AMSR2_{0}.tif'.format(\n                                                   datetime.strftime(ddate, '%Y%m%d')))):\n                urllib.request.urlretrieve(url, os.path.join(tmpDir, 'asi-AMSR2.tif'))\n                if (q == 'Y'):\n                    shutil.copy2(os.path.join(tmpDir, 'asi-AMSR2.tif'),\n                                 os.path.join(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters'),\n                                              'asi-AMSR2_{0}.tif'.format(\n                                                  datetime.strftime(ddate, '%Y%m%d'))))\n                    src_filename = os.path.join(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters'),\n                                                'asi-AMSR2_{0}.tif'.format(\n                                                    datetime.strftime(ddate, '%Y%m%d')))\n                else:\n                    src_filename = os.path.join(tmpDir, 'asi-AMSR2.tif')\n            else:\n                src_filename = os.path.join(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters'),\n                                            'asi-AMSR2_{0}.tif'.format(\n                                                datetime.strftime(ddate, '%Y%m%d')))\n            try:\n                outval = self.pointExtract(src_filename, dlat, dlon)\n                prm[4].put(prm[5])\n            except:\n                outval = np.nan\n                prm[4].put(prm[5])\n                pass\n        L.release()\n        return ([float(outval), datetime.strftime(ddate.to_pydatetime(), '%Y-%m-%d')])\n\n    def extractFromCSI(self, prm):\n        '''For each acquisition date in the csv file, differences are calculated between each one of them and each one included\n        in the CIS tar files. Then the tar file corresponding the minimum difference is choosen for the extraction\n        of the Sea Ice Concentration. '''\n        L.acquire()  # this is needed to lock each process to let them running separately without writing in a same variable at the same time\n        ddate = pd.to_datetime(prm[0])  # Acquisition date from the csv file.\n        dlat = prm[1]\n        dlon = prm[2]\n        fcis = prm[3]\n        CISRaster = prm[4]\n\n        CIS_acquisition_times = [datetime.strptime(i.split('_')[2], '%Y%m%dT%H%MZ').date() for i in fcis]\n        wq = np.array(CIS_acquisition_times)\n        CIS_acquisition_times = list(np.unique(wq))\n        csv_acquisition_time = datetime.strftime(ddate, '%Y%m%dT%H%M%S')\n        sample_date = datetime.strptime(csv_acquisition_time, '%Y%m%dT%H%M%S').date()\n        dt_abs = [abs(sample_date - each_date) for each_date in CIS_acquisition_times]\n        closest_date = CIS_acquisition_times[dt_abs.index(min(dt_abs))]\n        outraster = os.path.join(CISRaster, [i for i in fcis if i.__contains__(datetime.strftime(closest_date, '%Y%m%d'))][0][:-4] + '.tif')\n        intval = self.pointExtract(outraster, dlat, dlon)\n        outval = intval[0][0]\n        prm[5].put(prm[6])\n        L.release()  # this releases the locked process\n        return ([float(outval), datetime.strftime(closest_date, '%Y-%m-%d')])\n#         return ([float(outval), datetime.strftime(CIS_acquisition_times[s[0][0]], '%Y-%m-%d')])\n\n    def selectCISFiles(self, prm):\n        '''This function select the filenames from the CIS acquired at the nearest time as each one of the within the\n        input csv file. The output list of files will be used to download them. '''\n        L.acquire()\n        ddate = pd.to_datetime(prm[0])\n        fcislist = prm[3]\n        CIS_acquisition_times = [datetime.strptime(i.split('_')[2], '%Y%m%dT%H%MZ').date() for i in fcislist]\n        wq = np.array(CIS_acquisition_times)\n        CIS_acquisition_times = list(np.unique(wq))\n        u = datetime.strftime(ddate, '%Y%m%dT%H%M%S') # sample date\n        sample_date = datetime.strptime(u, '%Y%m%dT%H%M%S').date()\n        dt_abs = [abs(sample_date - each_date) for each_date in CIS_acquisition_times]\n        closest_date = CIS_acquisition_times[dt_abs.index(min(dt_abs))]\n        prm[5].put(prm[6])\n        L.release()\n        return [i for i in fcislist if i.__contains__(datetime.strftime(closest_date, '%Y%m%d'))][0]\n\n    def binaryretrieve(self, j):\n        '''The connection to the FTP server of the Canadian Ice Service is done in this function.\n        All the spatial coordinate are assumed to be within the region Eastern_Arctic and in the same year 2016\n        as the case of the GreenEdge data. '''\n        L.acquire()\n        i = j[0]\n        shp_for_UC3 = j[1]\n        hostname = 'sidads.colorado.edu'\n        ftp = FTP(hostname)\n        ftp.login(user='anonymous', passwd='')\n        ## This should be changed to be dynamically change depending on the region (here: Eastern_Arctic) and the year.\n        # The User Guide from the Canadian Ice Service (https://nsidc.org/data/G02171/versions/1?qt-data_set_tabs=3#qt-data_set_tabs) discribe all the possible region names\n        ftp.cwd('/pub/DATASETS/NOAA/G02171/Eastern_Arctic/2016/') \n        if not os.path.exists(os.path.join(shp_for_UC3, i)):\n            with open(os.path.join(shp_for_UC3, i), 'wb') as localfile:\n                ftp.retrbinary('RETR ' + i, localfile.write, 1024)\n        ftp.quit()\n        j[2].put(j[0])\n        L.release()\n        return 0\n\n    def fetchTarFromCIS(self, tarfile, shp_for_UC3, CISRaster, prglabel, pr, pStatus):\n        ''' This function manage the file retrieval from the CIS and then call to another function to do the vector\n        shapefiles into rasters. '''\n\n        if not os.path.exists(shp_for_UC3):\n            os.makedirs(shp_for_UC3)\n        '''For now, we assume all the data in the csv file were acquired in the same year (2016) and from\n        the same region (Eastern_Arctic) as the case of the GreenEdge data.'''\n        pool = mlp.Pool(processes=mlp.cpu_count()-2)\n        m = Manager()\n        queue = m.Queue()\n        tarfilelist = [[i, shp_for_UC3, queue] for i in tarfile]\n        s = pool.map_async(self.binaryretrieve, tarfilelist)\n        ##\n        while True:\n            if s.ready():\n                break\n            else:\n                c1 = int(queue.qsize() * 100 / len(tarfilelist))\n                pr.value=c1\n                prglabel.value = 'Fetching files from the CIS server...'\n                pStatus.value = f'{pr.value}%'\n        ##\n        del pool, s, queue, m\n        # Start a new progressbar for the shapefile converstion\n        m = Manager()\n        queue = m.Queue()\n        pr.value=0\n        prglabel.value = 'Converting shapefiles into raster files...'\n        pStatus.value = f'{pr.value}%'\n        ###################################################################\n        for f in tarfile:\n            '''Here the *.shp file have already been extracted from the *.tar file, \n            so we only need to point to it as shp_filename'''\n            shutil.unpack_archive(os.path.join(shp_for_UC3, f), shp_for_UC3, f[-3:])\n            shp_filename = os.path.join(shp_for_UC3, f[:-4] + '.shp')\n            outraster = os.path.join(CISRaster, f[:-4] + '.tif')\n            if not os.path.exists(outraster):\n                self.makeRasterFromSHP(shp_filename, outraster, 100)\n            queue.put(1)\n            c1 = int(queue.qsize() * 100 / len(tarfilelist))\n            pr.value=c1\n            pStatus.value = f'{pr.value}%'\n        del m, queue\n        return 0\n\n\n    def makeRasterFromSHP(self, shp_filename, outraster, pxlsize):\n        ''' This function handle the transformation of the vector shapefile format into rasters format.\n        The ogr python binding package is used to read the vector shepefile before their transfmation into raster.\n         Here only the Field CT holding the Sea Ice Concentration data are rasterized.\n         If needed, other Field amoung all of those included in the shapefile can be added as an additional band\n         into the output raster. '''\n        shpfile = ogr.Open(shp_filename)\n        layer = shpfile.GetLayer()\n        xmin, xmax, ymin, ymax = layer.GetExtent()\n        cols = int((xmax - xmin) / pxlsize)\n        rows = int((ymax - ymin) / pxlsize)\n        rdrive = gdal.GetDriverByName('GTiff')\n        ds = rdrive.Create(outraster, cols, rows, 1, gdal.GDT_Byte)\n        ds.SetGeoTransform([xmin, pxlsize, 0, ymax, 0, -pxlsize])\n        gdal.RasterizeLayer(ds, [1], layer, options=['ATTRIBUTE=CT'])\n        ds.SetProjection(layer.GetSpatialRef().ExportToPrettyWkt())\n        ds.GetRasterBand(1).SetNoDataValue(0)\n        del ds, rdrive, shpfile, layer\n        return 0\n\n    def pointExtract(self, src_filename, dlat, dlon):\n        ''' The extraction process is handled inside this function.\n        The GDAL python binding package is used here to read the raster files needed for the extraction.\n        The pyproj package is used to bring the geographical coordinates from the input csv to match the spatial\n         coordinate system of the rasters in order to make the extraction of the right collocated pixel with the csv data. '''\n        src_ds = gdal.Open(src_filename)\n        gt = src_ds.GetGeoTransform()\n        band = src_ds.GetRasterBand(1)\n        proj = osr.SpatialReference(wkt=src_ds.GetProjection())\n        #####\n        wgs84 = pyproj.CRS(\"EPSG:4326\")\n        rstProj = pyproj.CRS(proj.ExportToProj4())\n        #####\n        point = ogr.Geometry(ogr.wkbPoint)\n        point.AddPoint(float(dlat), float(dlon))  # to make sure the corrdinates are not in string format\n        mx, my = pyproj.Transformer.from_proj(wgs84, rstProj).transform(point.GetX(), point.GetY())\n        px = int((mx - gt[0]) / gt[1])  # x pixel\n        py = int((my - gt[3]) / gt[5])  # y pixel\n        intval = band.ReadAsArray(px, py, 1, 1)\n        del band, src_ds, point, proj, gt, mx, my, px, py\n        return intval\n\n    def getCISTarFileList(self, dlist, prglabel, pr, pStatus):\n        ''' This function extract the file list needed for the extraction. It uses the acquisition year\n        in the input csv file to locate the same year used in the FTP data endpoint in order to select the\n        files to be downloaded. '''\n        dl = pd.to_datetime(dlist)\n        csv_year = [datetime.strftime(s, '%Y') for s in dl]\n        csv_year = np.unique(csv_year)\n        fcis_gen = []\n        fcis = []\n        m = Manager()\n        queue = m.Queue()\n        for y in csv_year:\n            hostname = 'sidads.colorado.edu'\n            ftp = FTP(hostname)\n            ftp.login(user='anonymous', passwd='')\n            ''' There is more regions to be considered. Here the region Easter_Arctic is directly selected. '''\n            # TODO: Find a way to make automatic selection of the region of interest regarding the spatial extent of coordinate in the csv file used\n            ftp.cwd('/pub/DATASETS/NOAA/G02171/Eastern_Arctic/{0}/'.format(y))\n            files = ftp.nlst()  # This extract all the files within the folder named with the YEAR in.\n            ftp.quit()\n            fcis_gen.append([i for i in files if i.__contains__('cis')])\n        for i in fcis_gen:\n            fcis = fcis + i\n            queue.put(i)\n            p = int(queue.qsize()*100/len(fcis_gen))\n            pr.value=p\n            prglabel.value = 'Building file list...'\n            pStatus.value = f'{pr.value}%'\n        del fcis_gen, m, queue\n        return fcis\n\n    def ExtractionPixelValues(self, param_to_extract, q, rstsource, prglabel, pr, pStatus):\n        ''' * inputfile: Comma Separated Value (CSV) file with a header containning Date, Latitude, and Longitude.\n            * param_to_extract: name of the parameter to be extracted from the raster. This will become the name of\n            the new column that will be added to the initial csv file. Here it is about Sea_Ice_Concentration.\n            The format of the Date in the csv file should be mm/dd/yyyy.\n            The Latitude and Longitude Should be in full decimal format and their values are in the range [-180, 180].\n\n            * The Extraction Process is don using parallel computing to accelerate the process. Parallel processing is\n            very usefull here as the Extraction of the data coresponding to each rows are independent from each other.\n        '''\n\n        outfile = os.path.join(os.path.split(self.inputfile)[0],\n                               os.path.split(self.inputfile)[1][:-4] + '_{0}_{1}.csv'.format(param_to_extract,\n                                                                                             rstsource))\n        if os.path.exists(outfile):\n            os.remove(outfile)\n        df = pd.read_csv(self.inputfile, header=0, sep=',', parse_dates=True, skiprows=[1])\n        time_header=[i for i in df.columns if\n                     ((i.__contains__('Date')) or (i.__contains__('date')) or\n                      (i.__contains__('Time')) or (i.__contains__('time')))].pop()\n        if rstsource == 'BU':\n            m = Manager()\n            queue = m.Queue()\n            lon = [i for i in df.columns if i.__contains__('longitude')].pop()\n            lat = [i for i in df.columns if i.__contains__('latitude')].pop()\n            p = [[df[time_header][i], df[lat][i], df[lon][i], q, queue, i] for i in df.index]\n            if q == 'Y':\n                if not os.path.exists(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters')):\n                    os.makedirs(os.path.join(os.path.dirname(self.inputfile), 'BU_rasters'))\n            pool = mlp.Pool(mlp.cpu_count() - 2)\n            s = pool.map_async(self.extractFromUniBremenAMSR2, p)\n            ##\n            while True:\n                if s.ready():\n                    break\n                else:\n                    c1=int(queue.qsize()*100/len(p))\n                    pr.value=c1 \n                    pStatus.value = f'{pr.value}%'\n            print(queue.qsize())\n            ##\n            a = np.array(s.get())\n            del s\n            b = np.transpose(a)\n            df[param_to_extract] = b[0]\n            df[time_header] = b[1]\n            del pool, m, queue\n        else:\n            CISRaster = os.path.join(os.path.dirname(self.inputfile), 'CISraster')\n            if not os.path.exists(CISRaster):\n                os.makedirs(CISRaster)\n            ''' Retrieval from the CIS server of filename list corresponding to each dataset in the csv file'''\n            fcis = self.getCISTarFileList(df[time_header], prglabel, pr, pStatus)\n            m = Manager()\n            queue = m.Queue()\n            pr.value=0\n            ''' Selection of each shapefile with a closest acquisition time to each dataset of the csv file. '''\n            lon = [i for i in df.columns if i.__contains__('longitude')].pop()\n            lat = [i for i in df.columns if i.__contains__('latitude')].pop()\n            p = [[df[time_header][i], df[lat][i], df[lon][i], fcis, CISRaster, queue, i] for i in df.index]\n            pool = mlp.Pool(mlp.cpu_count() - 2)\n            imglist = pool.map_async(self.selectCISFiles, p)\n            prglabel.value = 'Selecting CIS file...'\n            while True:\n                if imglist.ready():\n                    break\n                else:\n                    c1=int(queue.qsize()*100/len(p))\n                    pr.value=c1\n                    pStatus.value = f'{pr.value}%'\n            imgarray0 = np.array(imglist.get())\n            imgarray1 = np.unique(imgarray0)\n            imglist0 = list(imgarray1)\n            CIS_shp = os.path.join(os.path.dirname(self.inputfile), 'CIS_shp')\n            if not os.path.exists(CIS_shp):\n                os.makedirs(CIS_shp)\n            del imgarray0, imgarray1, pool, m, queue\n            '''Fetching the files from the remote server'''\n            self.fetchTarFromCIS(imglist0, CIS_shp, CISRaster, prglabel, pr, pStatus)\n            pool = mlp.Pool(mlp.cpu_count() - 2)\n            m = Manager()\n            queue = m.Queue()\n            p = [[df[time_header][i], df[lat][i], df[lon][i], fcis, CISRaster, queue, i] for i in df.index]\n            s = pool.map_async(self.extractFromCSI, p)\n            ##\n            while True:\n                if s.ready():\n                    break\n                else:\n                    c1=int(queue.qsize()*100/len(p))\n                    pr.value=c1\n                    prglabel.value = 'Extracting SIC from newly created rasters...'\n                    pStatus.value = f'{pr.value}%'\n            del imglist0\n            ##\n            a = np.array(s.get())\n            del pool, m, queue\n            b = np.transpose(a)\n            df[param_to_extract] = b[0]\n            df['CIS_dates'] = b[1]\n            del s\n            if q == 'N':\n                shutil.rmtree(os.path.join(os.path.dirname(self.inputfile), 'CISraster'))\n                shutil.rmtree(os.path.join(os.path.dirname(self.inputfile), 'CIS_shp'))\n        df.to_csv(outfile,\n                  sep=',', index=False, header=1)\n        output_shp = os.path.split(self.inputfile)[1][:-4] + '_{0}_{1}_SHP'.format(param_to_extract, rstsource) # Name of the output shapefile\n        self.createShapefile(df, output_shp, time_header, prglabel)\n        prglabel.value = 'Processing Finished!!'\n        pStatus.value = f'{pr.value}%'\n        output_gpkg = os.path.split(self.inputfile)[1][:-4] + '_{0}_{1}_gpkg'.format(param_to_extract, rstsource) # Name of the output Geopackage (gpkg) file\n        if not os.path.exists(os.path.join(os.path.dirname(self.inputfile), output_gpkg)):\n            os.makedirs(os.path.join(os.path.dirname(self.inputfile), output_gpkg))\n        gpkgfile = os.path.join(os.path.dirname(self.inputfile), output_gpkg, output_gpkg + '.gpkg')\n        os.system(f'ogr2ogr -a_srs EPSG:4326 -oo X_POSSIBLE_NAMES=Lon* -oo Y_POSSIBLE_NAMES=Lat*  -f \"GPKG\" {gpkgfile} {outfile}') # Creates the gpkg file from the shapefile. This can be created directly from the csv.\n        return output_shp\n\n    def getSeaIceSource(self, workDir, r, gLocalCopy, prglabel, pr, pStatus):\n        ''' The choice between the Sea Ice Data from the Canadian Sea Ice Service and Bremen University is handled\n         in this function. When the choice is done, another function is called to take care of the process of\n         extracting Sea Ice Concentration from the chosen data source. '''\n        if not os.path.exists(self.inputfile):\n            print(\"Input file not exists !\")\n            exit(-1)\n        if r == 1:\n            r = 'BU'\n        elif r == 2:\n            r = 'CIS'\n        else:\n            exit(-1)\n        if gLocalCopy == 'y':\n            gLocalCopy = 'Y'\n        elif gLocalCopy == 'n':\n            gLocalCopy = 'N'\n        t = datetime.now()\n        output_shp = self.ExtractionPixelValues('sea_ice_co', gLocalCopy, r, prglabel, pr, pStatus)\n        print(datetime.now() - t)\n        return output_shp\n\nclass ccadi_uc3_mapping():\n    def __init__(self):\n        ## initiate the grid to display the contents of the page ###\n        self.gridwindow={}\n        self.vbox_widgets = []\n        self.gridwindow['grid'] = widgets.GridspecLayout(1,1)\n                \n        #####\n\n         # read text\n        f=open(os.environ[\"DATA_PATH\"] + \"/md_texts/SeaIceConcentration.md\",\"r\")\n        fc=f.read()\n        f.close()\n        text_html1 = markdown.markdown(fc)\n        del fc\n        self.gridwindow['InfoSIC'] = widgets.HTML(text_html1)\n        self.vbox_widgets.append(self.gridwindow['InfoSIC'])\n        # Fields\n        self.wdField = widgets.Text(\n            value=os.path.join(\"2016_int_btl_csv\", \"merged_btl_nutrient_pyco2sys.csv\"),\n            layout=widgets.Layout(width='max-content')\n        )\n\n        self.chkb1 = widgets.Checkbox(\n            value=False,\n            description='Canadian Ice Service',\n            disabled=False,\n            indent=False\n        )\n        self.chkb2 = widgets.Checkbox(\n            value=False,\n            description='Bremen University',\n            disabled=False,\n            indent=False\n        )\n        self.chkb3 = widgets.Checkbox(\n            value=False,\n            description='Keep a local copy of the raster images',\n            disabled=False,\n            indent=False\n        )\n        self.gridwindow['checkbox'] = widgets.VBox(children=[self.chkb1, self.chkb2, self.chkb3])\n        self.vbox_widgets.append(self.gridwindow['checkbox'])\n        self.prg = widgets.IntProgress(\n            value=0,\n            min=0,\n            max=100,\n            bar_style='success',\n            style={'bar_color': 'green'},\n            orientation='horizontal',\n            layout=widgets.Layout(width='800px')\n        )\n        self.status = widgets.Label(value=f'{self.prg.value}%', layout=widgets.Layout(width='max-content'))\n        self.prg_label = widgets.Label('', layout=widgets.Layout(width='max-content'))\n        self.vbox_widgets.append(self.prg_label)\n        self.gridwindow['progressbar'] = widgets.HBox(children=[self.prg, self.status])\n        self.vbox_widgets.append(self.gridwindow['progressbar'])\n        self.okButton = widgets.Button(description=\"OK\")\n        \n        self.okButton.on_click(self.clickOkbutton)\n        #####\n        \n        self.UC3_mapping()\n\n\n    def check_checkBox(self):\n        c = 0\n        value = ''\n        q = 'n'\n        if self.chkb1.value==True:\n            value = self.chkb1.description\n            c = 2\n        if self.chkb2.value==True:\n            value = self.chkb2.description\n            c = 1\n        if (self.chkb3.value==True):\n            q = 'y'\n        return c, q\n\n\n    def clickOkbutton(self, b):\n        self.prg.value=0\n        self.prg_label.value = 'Processing...'\n        workDir = os.path.dirname(self.wdField.value)\n        if not os.path.exists(workDir):\n            os.makedirs(workDir)\n        inputfile = self.wdField.value\n        r, q = self.check_checkBox()\n        output_shp = addSeaIceConcentration(inputfile).getSeaIceSource(workDir, r, q, self.prg_label, self.prg, self.status)\n        \n\n    def UC3_mapping(self): \n        def on_button_clicked(b):\n            showmap()\n\n        out=widgets.Output()\n        @out.capture()\n        def showmap():\n\n            workDir=os.path.join(\"2016_int_btl_csv\")\n            shp=os.path.join(workDir, \"merged_btl_nutrient_pyco2sys_sea_ice_co_CIS_SHP\", \"merged_btl_nutrient_pyco2sys_sea_ice_co_CIS_SHP.shp\")\n            data_full=gpd.read_file(shp)\n                        \n            # Create a Geo-id which is needed by the Folium (it needs to have a unique identifier for each row)\n            data_full['geoid'] = data_full.index.astype(str)\n\n#             dataf_0m=data.loc[np.round(data[\"sample_dep\"].values)==1]\n            dataf_10m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==10]\n            dataf_20m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==20]\n            dataf_30m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==30]\n            dataf_40m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==40]\n            dataf_50m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==50]\n            dataf_60m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==60]\n            dataf_70m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==70]\n            dataf_80m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==80]\n            dataf_90m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==90]\n            dataf_100m=data_full.loc[np.round(data_full[\"sample_dep\"].values)==100]\n\n            ###\n\n            lonCent = (data_full.bounds.maxx + data_full.bounds.minx).mean()/2\n            latCent = (data_full.bounds.maxy + data_full.bounds.miny).mean()/2\n            # creating a map object\n            m = leafmap.folium.Map(location=(latCent,lonCent), projections=\"epsg3575\", zoom_start=6)\n            #rst = os.path.join(\"2016_int_btl_csv\",\"CISraster\",\"cis_SGRDREA_20160606T1800Z_pl_a.tif\")\n\n            ###\n            # Create the variable plot upon click on the stations on the map\n            def chart_func(df, st): #new function\n                chart_temp = alt.Chart(df).mark_line(color='red').transform_fold(\n                    fold=['CTDTmp90', 'sample_dep'], \n                    as_=['variable', 'value']).encode(\n                        x=alt.X('CTDTmp90:Q', \n                                axis=alt.Axis(title='Temperature (C)', \n                                              titleColor='red'), \n                                scale=alt.Scale(domain=[df['CTDTmp90'].min(), \n                                                        df['CTDTmp90'].max()])),\n                        y=alt.Y('sample_dep:Q',\n                                axis=alt.Axis(title='Depth (m)'), \n                                scale=alt.Scale(reverse=True, \n                                                domain=[0, df['sample_dep'].max()])),\n                        color=alt.value('red')\n                )\n                chart_sal=alt.Chart(df).mark_line(color='green').transform_fold(\n                    fold=['P_sal_CTD', 'sample_dep'], \n                    as_=['variable', 'value']).encode(\n                        x=alt.X('P_sal_CTD:Q', \n                                axis=alt.Axis(title='Salinity', \n                                              titleColor='green'), \n                                scale=alt.Scale(domain=[df['P_sal_CTD'].min(), \n                                                        df['P_sal_CTD'].max()])),\n                        y=alt.Y('sample_dep:Q', \n                                axis=alt.Axis(title='Depth (m)'), \n                                scale=alt.Scale(reverse=True, \n                                                domain=[0, df['sample_dep'].max()])),\n                        color=alt.value('green')\n                )\n                ufchart=alt.layer(chart_temp, chart_sal, \n                                  title=f\"Vertical profil of Salinity and Temperature at Station: {st}\", \n                                  width=400, height=400).resolve_scale(x='independent').configure_axisTop(titleColor='green').configure_axisBottom(titleColor='red').resolve_legend(color='independent')            \n                return ufchart.to_json()\n            \n            # extract unique coordinates\n            data_full=data_full.round({'latitude':3, 'longitude':3})\n            df=data_full[['latitude', 'longitude']].drop_duplicates() # drop all duplicated coordinates and keep the row indexes\n            u=[]\n            for i in df.index:  # use the indexes (kept in the precedent lines) to build a new dataframe from df\n                u.append(data_full.values[i])\n            dg=pd.DataFrame(u, columns=data_full.columns)\n            data_coord=dg[['station', 'latitude', 'longitude']]\n            del dg\n            full_profile = leafmap.folium.FeatureGroup(name=\"Full profiles\")\n            for i, st in zip(df.index, data_coord['station'].values[:]):\n                ds0=data_full[\n                    [\n                        'sample_dep',\n                        'P_sal_CTD', \n                        'station', \n                        'CTDTmp90', \n                        'latitude', \n                        'longitude']\n                ].loc[\n                    data_full[\"station\"].values==st\n                ]\n                ds2=ds0.dropna().round({\n                    \"CTDTmp90\":2, \n                    \"P_sal_CTD\":2, \n                    'latitude': 3, \n                    'longitude':3})\n                chart=chart_func(ds0, st)\n                pp=leafmap.folium.Popup(max_width=600).add_child(leafmap.folium.VegaLite(chart, width=600))\n                full_profile.add_child(leafmap.folium.CircleMarker(\n                    location=[data_full['latitude'].values[i], data_full['longitude'].values[i]], radius=6,\n                    popup=pp,\n                ))\n            full_profile.add_to(m)\n            \n            # Select only needed columns           \n            data_10m = dataf_10m[['geoid', 'P_sal_CTD', 'station', 'sample_dep', 'CTDTmp90', 'geometry']]\n            \n            # Add data near the sea surface: 10m\n            leafmap.folium.features.GeoJson(dataf_10m,\n                                            name='Data at 10m depth',\n                                            style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                                            tooltip=leafmap.folium.features.GeoJsonTooltip(\n                                                fields=[\n                                                    'P_sal_CTD', \n                                                    'station', \n                                                    'sample_dep', \n                                                    'CTDTmp90'],\n                                                aliases = [\n                                                    'Practical salinity from CTD', \n                                                    'Station name', \n                                                    'sample depth (m)',\n                                                    'Temperature from CTD (C)'\n                                                ],\n                                                sticky=False)\n                                           ).add_to(m)\n\n\n            # Select only needed columns\n            data_20m = dataf_20m[['geoid', 'P_sal_CTD', 'station', 'sample_dep', 'CTDTmp90', 'geometry']]\n            \n            # Add data near the sea surface: 20m\n            leafmap.folium.features.GeoJson(dataf_20m,\n                                            name='Data at 20m depth',\n                                            style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n                                            tooltip=leafmap.folium.features.GeoJsonTooltip(\n                                                fields=[\n                                                    'P_sal_CTD', \n                                                    'station', \n                                                    'sample_dep', \n                                                    'CTDTmp90'],\n                                                aliases = [\n                                                    'Practical salinity from CTD', \n                                                    'Station name', \n                                                    'sample depth (m)',\n                                                    'Temperature from CTD (C)'\n                                                ],\n                                                sticky=False)\n                                           ).add_to(m)\n            \n            # Select only needed columns\n            data_30m = dataf_30m[['geoid', 'P_sal_CTD', 'station', 'sample_dep', 'CTDTmp90', 'geometry']]\n            \n            # Add data near the sea surface: 30m\n            leafmap.folium.features.GeoJson(dataf_30m,\n                                            name='Data at 30m depth',\n                                            style_function=lambda x: {\n                                                'color':'transparent',\n                                                'fillColor':'transparent',\n                                                'weight':0\n                                            },\n                                            tooltip=leafmap.folium.features.GeoJsonTooltip(\n                                                fields=[\n                                                    'P_sal_CTD', \n                                                    'station', \n                                                    'sample_dep', \n                                                    'CTDTmp90'],\n                                                aliases = [\n                                                    'Practical salinity from CTD', \n                                                    'Station name', \n                                                    'sample depth (m)',\n                                                    'Temperature from CTD (C)'\n                                                ],\n                                                sticky=False)\n                                           ).add_to(m)\n\n            # Select only needed columns\n            data_40m = dataf_40m[['geoid', 'P_sal_CTD', 'station', 'sample_dep', 'CTDTmp90', 'geometry']]\n            \n            # Add data near the sea surface: 40m\n            leafmap.folium.features.GeoJson(dataf_40m,\n                                            name='Data at 40m depth',\n                                            style_function=lambda x: {\n                                                'color':'transparent',\n                                                'fillColor':'transparent',\n                                                'weight':0\n                                            },\n                                            tooltip=leafmap.folium.features.GeoJsonTooltip(\n                                                fields=[\n                                                    'P_sal_CTD', \n                                                    'station', \n                                                    'sample_dep', \n                                                    'CTDTmp90'],\n                                                aliases = [\n                                                    'Practical salinity from CTD', \n                                                    'Station name', \n                                                    'sample depth (m)',\n                                                    'Temperature from CTD (C)'\n                                                ],\n                                                sticky=False)\n                                           ).add_to(m)\n#           # Select only needed columns\n            data_50m = dataf_50m[['geoid', 'P_sal_CTD', 'station', \n                                  'sample_dep', 'CTDTmp90', 'geometry', \n                                  'latitude', 'longitude']]\n    \n            # Add data near the sea surface: 50m\n            leafmap.folium.features.GeoJson(dataf_50m,\n                                            name='Data at 50m depth',\n                                            style_function=lambda x: {\n                                                'color':'transparent',\n                                                'fillColor':'transparent',\n                                                'weight':0\n                                            },\n                                            tooltip=leafmap.folium.features.GeoJsonTooltip(\n                                                fields=[\n                                                    'P_sal_CTD', \n                                                    'station', \n                                                    'sample_dep', \n                                                    'CTDTmp90'],\n                                                aliases = [\n                                                    'Practical salinity from CTD', \n                                                    'Station name', \n                                                    'sample depth (m)',\n                                                    'Temperature from CTD (C)'\n                                                ],\n                                                sticky=False)\n                                           ).add_to(m)\n\n#             ######################################################################################################################\n\n            leafmap.folium.LayerControl().add_to(m)\n            display(m)\n        \n        self.showmap_button=widgets.Button(\n                description='Show Map',\n                disabled=False,\n                button_style='', \n                tooltip='Click me',\n                icon=''\n                )\n        self.gridwindow['ok_and_continue'] = widgets.HBox(children=[self.okButton, self.showmap_button])\n        self.vbox_widgets.append(self.gridwindow['ok_and_continue'])\n        \n        self.gridwindow['grid'][0, 0] = widgets.VBox(children=self.vbox_widgets)  #\n        \n        self.accordion0 = widgets.Accordion(\n            children=[widgets.HBox(children = [self.gridwindow['grid'][0, 0]])]\n        )\n        self.accordion0.set_title(0, 'Adding Sea Ice Concentrations into the combined BTL_Nutrient file.')\n        display(self.accordion0)\n\n        self.showmap_button.on_click(on_button_clicked)\n        display(out)\n    \n\n\n    \n\n\n\nmerging_gui_jupiter()\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/ccadi-uc3","position":1},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI"},"type":"lvl1","url":"/notebooks/xcube-tutorial","position":0},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI"},"content":"","type":"content","url":"/notebooks/xcube-tutorial","position":1},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Introduction"},"type":"lvl3","url":"/notebooks/xcube-tutorial#introduction","position":2},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Introduction"},"content":"This notebook uses the xcube function to call imagery into a cube, with the axis being lat, lon, and time. Each point within the cube has whatever bands you call into it. It has its pros and cons, as it certainly cannot do as much as the API but it is significantly easier to use and understand.\n\n","type":"content","url":"/notebooks/xcube-tutorial#introduction","position":3},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Learning Objectives"},"type":"lvl3","url":"/notebooks/xcube-tutorial#learning-objectives","position":4},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Learning Objectives"},"content":"By the end of the lab exercise, you should be able to:\n\nImport supporting code packages into your project\n\nAccess PolarTEP data for analysis\n\nCreate a multivariable data cube\n\nVisualize data\n\nAnalyze data and generate new data (Normalized Difference Vegetation Index - NDVI)\n\nCreating ancillary data visualizations\n\nExporting data for use on other platforms\n\n","type":"content","url":"/notebooks/xcube-tutorial#learning-objectives","position":5},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Required skills and prerequisites"},"type":"lvl3","url":"/notebooks/xcube-tutorial#required-skills-and-prerequisites","position":6},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Required skills and prerequisites"},"content":"To complete this tutorial you should have the following skills and tools:\n\nThe latest version of a modern browser (e.g. Google Chrome, Mozilla Firefox, Microsoft Edge)\n\nA current PolarTEP account\n\nBasic understanding of Python code\n\nAbility to use a Jupyter Notebook, including running code blocks\n\nCompletion of introductory PolarTEP tutorials (INSERT HERE)\n\n","type":"content","url":"/notebooks/xcube-tutorial#required-skills-and-prerequisites","position":7},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"External References"},"type":"lvl3","url":"/notebooks/xcube-tutorial#external-references","position":8},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"External References"},"content":"Python for Beginners\n\nJupyter Notebook documentation\n\nNormalized Difference Vegetation Index\n\n","type":"content","url":"/notebooks/xcube-tutorial#external-references","position":9},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Importing supporting code packages into your project"},"type":"lvl3","url":"/notebooks/xcube-tutorial#importing-supporting-code-packages-into-your-project","position":10},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Importing supporting code packages into your project"},"content":"Most projects will benefit from the use of existing code libraries or packages that add functionality to your application.  In this case, you will be creating a data cube, using data arrays, and other data manipulation and analysis utilties.\n\nfrom edc import check_compatibility\ncheck_compatibility(\"user-2022.10-14\", dependencies=[\"SH\"])\n\n#Packages \n\n# xcube_sh imports\nfrom xcube_sh.cube import open_cube\nfrom xcube_sh.config import CubeConfig\nfrom xcube_sh.observers import Observers\nfrom xcube_sh.viewer import ViewerServer\n\n# xcube imports\nfrom xcube.core.maskset import MaskSet\nfrom xcube.core.geom import mask_dataset_by_geometry\nfrom xcube.core.geom import clip_dataset_by_geometry\n\n#import xarray as xr \nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rasterio\n#import odc.geo.xr\n\n# Various utilities\nimport json\nfrom datetime import date, timedelta\nimport xarray as xr\nimport shapely.geometry\nimport IPython.display\nimport zarr\nimport numpy as np\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#importing-supporting-code-packages-into-your-project","position":11},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Data Generation (Accessing PolarTEP data for analysis)"},"type":"lvl3","url":"/notebooks/xcube-tutorial#data-generation-accessing-polartep-data-for-analysis","position":12},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Data Generation (Accessing PolarTEP data for analysis)"},"content":"The first step in creating an application that analyzes data is to access data.  Using a data cube to generate an NDVI is inherhently a geospatial data analysis.  A study area or bounding box must first be defined.\n\n#Creates bounding box, edit the coordinates to change the area\nx1 = -134.67 # degrees, westmost point\ny1 = 69.41  # degrees, southmost point\nx2 = -134.54  # degrees, eastmost point\ny2 = 69.50  # degrees, northmost point\n\nbbox = x1, y1, x2, y2\n\n\n\nAn IPython function can be used to visualize your study area (bounding box)\n\nIPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__)\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#data-generation-accessing-polartep-data-for-analysis","position":13},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Generate Cube (create a multivariate data cube)"},"type":"lvl3","url":"/notebooks/xcube-tutorial#generate-cube-create-a-multivariate-data-cube","position":14},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Generate Cube (create a multivariate data cube)"},"content":"First create the configurations required for the cube. This includes the data bands and dates.\n\nMore information on S2L2A and other SH functionalities: \n\nhttps://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/\n\nspatial_res = 0.00018   # = 10.038 meters in degrees\n\ntime_range = ['2018-08-30','2018-09-30']\ncube_config = CubeConfig(dataset_name = 'S2L2A', #Calls the Sentinel-2 L2A library\n                         band_names = ['B02', 'B03', 'B04', 'B08'],\n                         tile_size = [512, 512],\n                         bbox = bbox,\n                         spatial_res = spatial_res,\n                         time_range = time_range,\n                         time_period = '5D') #S2L2A has a revisit time of 5 days, so anything shorter would be redundant\n\n\n\nThen create the cube. For computational efficiency the cube is created as a shell, and the data is called when required.\n\nrequest_collector = Observers.request_collector() #Sends data to SH\n\n#Create the actual cube containing 'lazy' data\ncube = open_cube(cube_config, observer=request_collector)\n\n\n\n#Display cube\ncube\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#generate-cube-create-a-multivariate-data-cube","position":15},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Visualize data from the cube"},"type":"lvl3","url":"/notebooks/xcube-tutorial#visualize-data-from-the-cube","position":16},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Visualize data from the cube"},"content":"\n\n#Display RGB image of a single day\ncube[['B04', 'B03', 'B02']].isel(time=3).to_array().plot.imshow(robust=True, figsize=(10,10))\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#visualize-data-from-the-cube","position":17},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Analyze data and generate new data (Normalized Difference Vegetation Index - NDVI)"},"type":"lvl2","url":"/notebooks/xcube-tutorial#analyze-data-and-generate-new-data-normalized-difference-vegetation-index-ndvi","position":18},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Analyze data and generate new data (Normalized Difference Vegetation Index - NDVI)"},"content":"\n\n#Create NDVI variable and add it into the cube\nndvi=(cube.B08-cube.B04)/(cube.B08+cube.B04)\n\nndvi.attrs['long_name'] = 'Normalized Difference Vegetation Index'\nndvi.attrs['units'] = 'unitless'\n\ncube['NDVI']=ndvi\n\n\n\n#Displaying NDVI from a single day\ncube.NDVI.sel(time='2018-09-30', method='nearest').plot.imshow(vmin=-1, vmax=1, cmap='RdYlGn', figsize=(13, 10))\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#analyze-data-and-generate-new-data-normalized-difference-vegetation-index-ndvi","position":19},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Creating ancillary data visualizations (graphs and charts)","lvl2":"Analyze data and generate new data (Normalized Difference Vegetation Index - NDVI)"},"type":"lvl3","url":"/notebooks/xcube-tutorial#creating-ancillary-data-visualizations-graphs-and-charts","position":20},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl3":"Creating ancillary data visualizations (graphs and charts)","lvl2":"Analyze data and generate new data (Normalized Difference Vegetation Index - NDVI)"},"content":"\n\n#Calculate the average using the x and y dimensions, so that it is done per day on the entire area\navg = cube.NDVI.mean(['lon', 'lat'])\n\n#Check to ensure the formatting is correct\nprint(avg)\n\n\n\n#Connect the NDVI value with the corrosponding date\ntimeseriesNDVI = avg.to_series()\n\n#Check to ensure the transpose occured correctly\nprint(timeseriesNDVI)\n\n\n\n#Simple plot to visualize the average NDVI change over time\nplt.plot(timeseriesNDVI)\nplt.xticks(rotation='vertical')\nplt.ylabel('NDVI')\nplt.show()\n\n\n\ncube.NDVI.plot.hist()\nplt.title('')\nplt.ylabel('Frequency')\nplt.xlabel('NDVI')\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#creating-ancillary-data-visualizations-graphs-and-charts","position":21},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Displaying all the dates"},"type":"lvl2","url":"/notebooks/xcube-tutorial#displaying-all-the-dates","position":22},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Displaying all the dates"},"content":"\n\ncube.NDVI.plot.imshow(col='time', col_wrap=5, vmin=-1, vmax=1,cmap='RdYlGn')\n\n\n\n\n\n# SEEMS TO HAVE FIXED THE NAN ISSUE WHEN USING DIFFERENT DATA\n\nfor ndvi_date in cube.NDVI:\n    print(\n        {\n            \"date\": str(ndvi_date.time.values),\n            \"max\": ndvi_date.values.max(),\n            \"mean\": ndvi_date.values.mean(),\n            \"min\": ndvi_date.values.min(),\n            \"stDev\": np.std(ndvi_date.values),\n        }\n    )\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#displaying-all-the-dates","position":23},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Discussion"},"type":"lvl2","url":"/notebooks/xcube-tutorial#discussion","position":24},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Discussion"},"content":"Student discussion goes here.  Possibly using interactive learning and evaluation tools.\n\n","type":"content","url":"/notebooks/xcube-tutorial#discussion","position":25},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Export data"},"type":"lvl2","url":"/notebooks/xcube-tutorial#export-data","position":26},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Export data"},"content":"\n\nimport os\n\ndel cube.attrs['history']\ncube.to_netcdf(path=os.path.expanduser(\"~/test_data.nc\"), mode='w', format='NETCDF4')\n\n\n\n","type":"content","url":"/notebooks/xcube-tutorial#export-data","position":27},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Quiz"},"type":"lvl2","url":"/notebooks/xcube-tutorial#quiz","position":28},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Quiz"},"content":"What are some limitations to the cube method?\n-- No cloud cover control, mosaicking is difficult and finicky, making it difficult to get the best possible image over a timeframe\n\nWhat are some of the benefits?\n-- Simple and easy to understand and manipulate. Can easily write to netCDF and put into GIS as a multilayer raster if you want to create better maps\n\nCalculate NDWI instead of NDVI, or other calculations within the actual cube\n\n","type":"content","url":"/notebooks/xcube-tutorial#quiz","position":29},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Related Research"},"type":"lvl2","url":"/notebooks/xcube-tutorial#related-research","position":30},{"hierarchy":{"lvl1":"PolarTEP eScience Lab:  Using XCube in Polar-TEP to Generate NDVI","lvl2":"Related Research"},"content":"Ashok, A., Rani, H. P., & Jayakumar, K. V. (2021). Monitoring of dynamic wetland changes using NDVI and NDWI based landsat imagery. Remote Sensing Applications: Society and Environment, 23, 100547.\n\nOloyede, A. O., Olatunbosun, D. E., Asuquo, P. M., Udo, U. E., & Essien, I. O. (2021). Correlation Analysis of Vegetation and Land Surface Temperature in Uyo, Nigeria Using Satellite Remote Sensing and Python-Based Geographic Information System. Science and Technology Publishing, 5(2632-1017).\n\n...","type":"content","url":"/notebooks/xcube-tutorial#related-research","position":31}]}